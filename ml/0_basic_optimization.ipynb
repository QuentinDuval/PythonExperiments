{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Gradient descent\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Basic algorithm\n",
    "\n",
    "The gradient of a function $f$ always point toward the direction with the steepest ascent. The gradient descent consists in following the reverse of the gradient to follow the steepest descent:\n",
    "\n",
    "&emsp; $f(x+v) \\simeq f(x) + v^T \\nabla f(x)$\n",
    "&emsp; $\\implies$\n",
    "&emsp; descent is steepest when $v$ are $\\nabla f(x)$ are in opposite direction\n",
    "\n",
    "The algorithm therefore proceeds in making a small step in the opposite direction of the gradient:\n",
    "\n",
    "&emsp; $x_{new} \\leftarrow x_{old} - \\alpha \\nabla f$\n",
    "&emsp; where\n",
    "&emsp; $\\alpha$ is called the learning rate\n",
    "\n",
    "This process is stopped when the gradient reaches 0 (critical point). We should not this does ensure we reached a global minimum. It might even in fact not bring us to any local minimum either, as we might stay stuck on a saddle point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Line search\n",
    "\n",
    "What is the size of the step we should perform? Note that in the previous algorithm, there is one key critical fact: **the larger the gradient, the larger the step**. It might seem counterintuitive, as we might expect that in a region of big variations, we should be more careful about our steps. On the contrary, when variations are small, we might want to do big steps, but that might also make us miss a close critical point.\n",
    "\n",
    "The thing is that there is not good answer here. One approch is to do a **line search**, that is try different step size:\n",
    "\n",
    "1. We first double the step size every time until it does not decrease anymore: $\\alpha_0, \\dots \\alpha_n$\n",
    "2. We then binary search for the best step size in the range $\\alpha_{n-1}$ to $\\alpha_n$\n",
    "\n",
    "This requires us to be able to evaluate $f(x - \\alpha_i \\nabla f)$ at each step. When too costly, this approch is best avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "The most famous algorithm for Artificial Neural Network of the past years. The idea is to avoid computing the exact gradient $\\nabla f$, but instead compute an estimate of the gradient $\\tilde{\\nabla f}$ and use it to perform our gradient descent:\n",
    "\n",
    "&emsp; $x_{new} \\leftarrow x_{old} - \\alpha \\tilde{\\nabla f}$\n",
    "&emsp; where\n",
    "&emsp; $\\alpha$ is called the learning rate\n",
    "\n",
    "Normally, the quantity we are trying to diminish is computed using the full training data. For instance, in the case of a regression, we are interested in minimizing the square distance of all points to their predicted outputs. SGD is obtained by **random sub-selection of the training data** to estimate of the gradient, for example by taking a mini-batch of 500 inputs among the 60,000 data points in input.\n",
    "\n",
    "More sample points means the estimate of the gradient gets better but also more expensive to compute. The random nature of the sub-selection might also help us **avoiding saddle points**: fewer points means more fluctuations and more chances to escape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Example with regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line search vs fixed learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Newton method\n",
    "---\n",
    "\n",
    "* Newton method in single variable\n",
    "* Newton method in multi variable calculus (hessian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Simulated Annealing / Beam Search / Genetic Algorithms\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

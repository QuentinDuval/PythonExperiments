{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Column and Vector spaces\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Column space\n",
    "\n",
    "The \"good\" way to see matrix multiplication: take each column of the right matrix and use these numbers as a linear combinations of the column vectors of the matrix.\n",
    "\n",
    "&emsp; $\\begin{pmatrix} u_1 & u_2 & ... & u_n \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ ... \\\\ x_n \\end{pmatrix} = x_1 u_1 + x_2 u_2 + ... + x_n u_n$\n",
    "\n",
    "We can see the column space as the space generated by column vectors, and we can see the **column vectors as the projection of the current basis vectors, encoded in the basis of the source space**.\n",
    "\n",
    "Note that the formula above is also valid in case $x_i$ are replaced by row vectors $v_i^T$:\n",
    "\n",
    "&emsp; $\\begin{pmatrix} u_1 & u_2 & ... & u_n \\end{pmatrix} \\begin{pmatrix} v_1^T \\\\ v_2^T \\\\ ... \\\\ v_n^T \\end{pmatrix} = u_1 v_1^T + u_2 v_2^T + ... + u_n v_n^T$\n",
    "\n",
    "It shows that we can decompose any matrix multiplication $AB$ where $A \\in R^{m \\times k}$ and $B \\in R^{k \\times n}$, as the sum of the product of the columns of the first matrix with the rows of the second matrix:\n",
    "\n",
    "&emsp; $AB = \\sum_k k^{th}\\text{ col of A} \\times k^{th}\\text{ row of B} = \\sum_k A_{[:,k]} B_{[k,:]} = \\sum_k \\text{rank 1 matrices}$\n",
    "\n",
    "Which is compliant to the classical rule: $(AB)_{ij} = \\sum_k A_{ik} B_{kj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Column space rank\n",
    "\n",
    "We can find the rank of a matrix by looking at its columns one by one, adding them to a new matrix if they are not a linear combinations of the other columns. For each column added, we complete a matrix that gives the linear combinations if they exist. This is illustrated below:\n",
    "\n",
    "&emsp; $A = \\begin{pmatrix} 2 & 1 & 3 \\\\ 3 & 2 & 5 \\\\ 5 & 7 & 12 \\end{pmatrix} \\rightarrow A = \\begin{pmatrix} 2 & 1 \\\\ 3 & 2 \\\\ 5 & 7 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} = C R$\n",
    "\n",
    "$R$ represent the linear combinations necessary to build the matrix $A$ from the matrix $C$:\n",
    "\n",
    "1. take the first row of C\n",
    "2. take the second row of C\n",
    "3. take the sum of the first and second row of C\n",
    "\n",
    "Interestingly, we can read this product the other way around, as $C^T$ being the linear combinations to perform on the rows of $R$ to obtain the matrix $A$ (this can easily be seen by transposing the product):\n",
    "\n",
    "1. take 2 times the first row of $R$ plus 1 times the second row of $R$\n",
    "2. take 3 times the first row of $R$ plus 2 times the second row of $R$\n",
    "3. take 5 times the first row of $R$ plus 7 times the second row of $R$\n",
    "\n",
    "This demonstrates one big theorem of Linear Algebra: the rank of the column space of $A$ is equal to the rank of the row space of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Matrix identities\n",
    "---\n",
    "\n",
    "**todo** useful transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Matrix decompositions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LU decomposition\n",
    "\n",
    "A matrix $A$ can be decomposed as $A = P L U$ where $P$ is a permutation matrix, $L$ is a lower diagonal matrix, $U$ is a upper diagonal matrix. The permutation matrix is not necessary. Example:\n",
    "\n",
    "&emsp; $A = \\begin{pmatrix} 1 & 2 & -1 \\\\ 4 & 5 & 0 \\\\ -2 & -1 & 4 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ -0.5 & 1 & 0 \\\\ 0.25 & 0.5 & 1 \\end{pmatrix} \\begin{pmatrix} 4 & 5 & 0 \\\\ 0 & 1.5 & 4 \\\\ 0 & 0 & -3 \\end{pmatrix}$\n",
    "\n",
    "The goal of this decomposition $A = LU$ is to **facilitate the resolution of systems of linear equations** $Ax = b$. Indeed, we can solve these systems using Gaussian Elimination, but this requires $O(N^3)$ steps, with $N$ being the number of rows or columns, because of $O(N)$ row manipulations that each go through half the matrix in average.\n",
    "\n",
    "In case of repeating evaluation for different $b$, decomposing the matrix $A = PLU$ allows to solve the system more quickly. Indeed, we can solve the system by exploiting the fact that $Ax = P(L(Ux)) = b$:\n",
    "\n",
    "1. Solve $Pz = b$: we can just permute the rows of $b$ in $O(N)$\n",
    "2. Solve $Ly = z$: we can get the value of $y$ in $O(N^2)$ by substitution since the gaussian elimination is already done\n",
    "3. Solve $Ux = y$: we can get the value of $x$ in $O(N^2)$ by substitution since the gaussian elimination is already done\n",
    "\n",
    "The idea of the LU decomposition is to see $Ax = b$ as $IAx = b$ and then perform scaling or row addition operations on $A$ as we do for Gaussian elimination, which can be represented as matrices $E_i$, and apply the reverse transformation on $I$:\n",
    "\n",
    "&emsp; $IAx=b \\implies (IE_0^{-1})(E_0A)x = b \\implies (E_0^{-1} .. E_n^{-1})(E_n .. E_0 A)x = b \\implies LUx=b$\n",
    "\n",
    "The technical trick we can use to do so is to draw the matrices $A$ and $I$ side by side, and doing the reverse operations:\n",
    "\n",
    "* divising by $k$ the row of $I$ when $A$'s row is multiplied by $k$\n",
    "* substracting row $k$ to row $j$ in $I$ when $A$'s row $k$ is adding to row $j$ (only for eliminated column)\n",
    "\n",
    "**Example**:\n",
    "\n",
    "We start with our $U$ matrix being equal to $A$, and $L$ as the identity matrix:\n",
    "\n",
    "&emsp; $U = \\begin{pmatrix} 1 & 2 & -1 \\\\ 4 & 5 & 0 \\\\ -2 & -1 & 4 \\end{pmatrix} L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$\n",
    "\n",
    "Gaussian elimination of the first column:\n",
    "\n",
    "&emsp; $U = \\begin{pmatrix} 1 & 2 & -1 \\\\ 0 & -3 & 4 \\\\ 0 & 3 & 2 \\end{pmatrix} L = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\boldsymbol{4} & 1 & 0 \\\\ \\boldsymbol{-2} & 0 & 1 \\end{pmatrix}$\n",
    "\n",
    "Gaussian elimination of the second column:\n",
    "\n",
    "&emsp; $U = \\begin{pmatrix} 1 & 2 & -1 \\\\ 0 & -3 & 4 \\\\ 0 & 0 & 6 \\end{pmatrix} L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 4 & 1 & 0 \\\\ -2 & \\boldsymbol{-1} & 1 \\end{pmatrix}$\n",
    "\n",
    "Here is how we can do it in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "[[ 1.    0.    0.  ]\n",
      " [-0.5   1.    0.  ]\n",
      " [ 0.25  0.5   1.  ]]\n",
      "[[ 4.   5.   0. ]\n",
      " [ 0.   1.5  4. ]\n",
      " [ 0.   0.  -3. ]]\n",
      "[[ 1.  2. -1.]\n",
      " [ 4.  5.  0.]\n",
      " [-2. -1.  4.]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 2, -1],\n",
    "    [4, 5, 0],\n",
    "    [-2, -1, 4]])\n",
    "\n",
    "permutation, lower_diag, upper_diag = scipy.linalg.lu(A)\n",
    "print(permutation)\n",
    "print(lower_diag)\n",
    "print(upper_diag)\n",
    "print(permutation @ lower_diag @ upper_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Diagonalisation\n",
    "\n",
    "A matrix of rank $N$ always has $N$ non-zero complex eigen values. Real eigen values correspond to scaling. Complex eigen values correspond to rotations (plus scaling). Such matrices can be decomposed as:\n",
    "\n",
    "&emsp; $A = U \\Lambda U^{-1}$, where $U$ are the eigen vectors, and $\\Lambda$ contains the eigen values $\\Lambda_{ii} = \\lambda_i$\n",
    "\n",
    "Let's prove it! For each eigen vector $v_i$, we have $A v_i = \\lambda_i v_i$. If we assemble these eigen vectors into a matrix, we get the formula:\n",
    "\n",
    "&emsp; $A U = U \\Lambda$ (each column of  Î›  multiplies the only eigen vector it matches) $\\implies A = U \\Lambda U^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Symmetric matrices\n",
    "\n",
    "Symmetric matrices have real eigenvalues $\\lambda_i$ and we can always find orthonormal eigenvectors $q_i$ that form a basis of their column space. If we assemble the vectors $q_i$ as column vectors of a matrix $Q$, we have the factorization:\n",
    "\n",
    "&emsp; $S = Q \\Lambda Q^T$, where $\\Lambda$ contains the eigen values $\\Lambda_{ii} = \\lambda_i$\n",
    "\n",
    "Let's prove it! For each eigen vectors $q_i$, we have $S q_i = \\lambda_i q_i$. If we assemble these eigen vectors into a matrix, we get the formula (where $\\Lambda$ is diagonal with $\\Lambda_{ii} = \\lambda_i$):\n",
    "\n",
    "&emsp; $S Q = Q \\Lambda$ (each column of $\\Lambda$ multiplies the only eigen vector it matches)\n",
    "\n",
    "&emsp; $S = Q \\Lambda Q^T$ (since $Q Q^T = I$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Singular Value Decomposition\n",
    "\n",
    "For any matrix, of any shape, we can always decompose the matrix as: $A = U \\Sigma V^T$, where:\n",
    "\n",
    "* $V$ is orthogonal and contains the eigen values of $A^T A$, with eigen values $\\lambda_i$\n",
    "* $U$ is orthogonal and contains the eigen values of $A A^T$\n",
    "* $\\Sigma$ is diagonal and contains the singular values $\\Sigma_{ii} = \\sigma_i$ such that $\\sigma_i^2 = \\lambda_i$\n",
    "\n",
    "Let's prove it! We know that $A^T A$ is symmetric, and so we can always decompose it as:\n",
    "\n",
    "&emsp; $A^T A = V \\Lambda V^T$\n",
    "\n",
    "If we take the vectors $v_i$ of $V$ and multiply these vectors to $A$, we get something of that form:\n",
    "\n",
    "&emsp; $A v_i = \\sigma_i u_i$, where $||u_i|| = 1$\n",
    "\n",
    "Now we can prove that the vectors $u_i$ are orthonormal:\n",
    "\n",
    "&emsp; $(A v_j)^T A v_i = v_j^T A^T A v_i = \\lambda_i v_j^T v_i = \\lambda_i \\delta_{ij}$ (because $v_i$ is an eigen vector of $A^T A$)\n",
    "\n",
    "&emsp; $(A v_j)^T A v_i = \\sigma_j \\sigma_i u_j^T u_j \\implies u_j^T u_i = \\delta_{ij}$ and $\\lambda_i = \\sigma_i^2$\n",
    "\n",
    "And so, by bulking the products for all the eigen vectors, we get:\n",
    "\n",
    "&emsp; $A V = U \\Sigma \\implies A = U \\Sigma V^T$ (because $V$ is orthnormal)\n",
    "\n",
    "Finally, we can prove that $U$ contains eigen vectors of $A A^T$ by multiplying the whole:\n",
    "\n",
    "&emsp; $A A^T = U \\Sigma V^T V \\Sigma^T U^T = U \\Sigma \\Sigma^T U^T$\n",
    "\n",
    "&emsp; $A A^T U = U \\Lambda'$ where $\\Lambda' = \\Sigma \\Sigma^T$ is diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

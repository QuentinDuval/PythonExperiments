{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Parametric vs Non-parametric\n",
    "---\n",
    "\n",
    "One way to approach learning is by trying to fit a model with parameters to the data that we observe, this is called the parametric approach to learning. Non-parametric approaches do not try to fit parameters but instead try to look for similarity between new inputs and training samples to take decisions. We discuss some examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Classification\n",
    "\n",
    "We have a number of samples as inputs with their respective classification $\\{x, c\\}_n$ and would like to classify new points.\n",
    "\n",
    "A parametric approach would consists in creating a model with parameters (for instance a linear classifier or a neural network) and optimize its parameters value through Maximum Likelihood (ML), Maximum A Posteriori (MAP) or Bayesian approaches. Once the parameters are learned, we can run the model on new data to classify it.\n",
    "\n",
    "An example of non parametric approach to classifying a new point $x$ by looking at the K-closest examples $\\{x, c\\}_k$ and take the class that appears the most. This approach requires to select a metric as well as keeping in memory all the training samples.\n",
    "\n",
    "**Non-parametric approaches do not forbid to have a training phase**. For instance, another non-parametric approach would be to run a K-mean algorithm to identify the main clusters of data, class by class, and compress this information to centroids. The K-closest approach above could be based on proximity with centroids instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Density estimation\n",
    "\n",
    "We have a number of samples $\\{x\\}_n$ that represents for instance the time of arrival of bus in a bus station. We would like to build a model that approximate this generating process for another inference task.\n",
    "\n",
    "A parametric approach would consists in creating a model (for instance, an exponential distribution with unknown parameter $\\lambda$) and fit this model using again the familiar ML, MAP or Bayesian approaches. Once the distribution $p(x)$ is known, we can use it to generate new samples.\n",
    "\n",
    "A non parametric approach could for instance rely on the law of large numbers. Inside a small volume of size $V$, count the number of elements $N_V$. The probability of generating a sample inside this volume is approximated to be equal to $p(x) = \\frac{N_V}{V}$.\n",
    "\n",
    "This approach leads to bining and is the basis for **histograms**: to generate a new sample, we select a volume based on its probability $\\frac{N_V}{N}$ and then pick uniformly a point inside this volume. Another similar approach, this time to estimate the probability of a given point $x$, is to look for a small volume around a point $x$ to estimate its probability:\n",
    "\n",
    "&emsp; $\\displaystyle p(x) = \\frac{1}{N h^D} \\sum_n k \\big( \\frac{x - x_n}{h} \\big)$\n",
    "&emsp; where:\n",
    "&emsp; $k(u) = \\delta (\\forall i, |u_i| \\le 0.5)$\n",
    "&emsp; $h$ is the width of the cube\n",
    "&emsp; $D$ is the dimensionality of $x$\n",
    "\n",
    "The main problem with these approaches is the **curse of dimensionality**: to be precise and lead to smooth density functions, this methods requires a high number of regions of size $V$ which grows exponentially with the number of dimensions.\n",
    "\n",
    "A more advanced approach is to approximate the regions of high densities with gaussians kernel, leading to:\n",
    "\n",
    "&emsp; $\\displaystyle p(x) = \\frac{1}{N} \\sum_n \\frac{1}{\\sqrt{2 \\pi h^2}} e^{\\textstyle - \\frac{\\Vert x - x_n \\Vert^2}{2 h^2} }$\n",
    "&emsp; where\n",
    "&emsp; $h$ controls the influence reach of each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Regression\n",
    "\n",
    "We have a number of samples with their target values $\\{x, t\\}_n$ and we wish to return the value $t$ associated with new data points $x$. This problem is actually pretty linked to density estimation: we could see it as learning $p(x,t)$ from which we can deduce $p(t|x)$.\n",
    "\n",
    "A parametric approach would consists in building a model (linear model, artificial neural network) with parameters $\\theta$ and to maximize (for instance) the maximum likelihood of the model $\\prod p(t_n|x_n,\\theta)$ with respect to $\\theta$ to find the parameters $\\theta$.\n",
    "\n",
    "A non parametric approach could consists in comparing the new data point $x$ to the sample points $x_n$, and use these similarity measure to assign it a linear combination of the values $t_n$:\n",
    "\n",
    "&emsp; $\\displaystyle y = \\sum_n k(x, x_n) t_n$\n",
    "&emsp; which is true only if $\\forall i, j, \\; k(x_i, x_j) = \\delta_{ij}$ (orthogonality)\n",
    "&emsp; $\\iff$\n",
    "&emsp; $K = X X^T = I_n$\n",
    "\n",
    "Why? Because **the $\\{t\\}_n$ must be decomposed on the basis** $\\{x\\}_n$, as there are highly correlated to each other for two points that are similar. Decomposing a vector $x$ on the basis $U = (u_1, \\dots u_n)$ is equivalent to finding the vector $y$ expressed in the basis $U$ such that $x = U y$, and so $y = U^{-1} x$. In our case, the basis to decompose our $\\{x\\}_n$ on is $K = X X^T$:\n",
    "\n",
    "&emsp; $t'_n = K^{-1} t_n$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $\\displaystyle t = \\sum_n k(x, x_n) K^{-1} t_n$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $\\boxed{y = k(x)^T K^{-1} t}$\n",
    "&emsp; where $k(x) = \\big(k(x,x_1), \\dots k(x, x_n) \\big)$\n",
    "\n",
    "This method is the basis of what we call **kernel methods** and **gaussian processes** which we will explore in future sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Examples\n",
    "---\n",
    "\n",
    "* classification of points\n",
    "* arrival of passengers\n",
    "* regression on simple curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The basis for a framework of automatic differentiation such as PyTorch.\n",
    "\n",
    "What we want is to:\n",
    "1. Build a forward pass manually, passing tensors / variable through blocks\n",
    "2. Make sure each variable remember which block uses it (to get its gradients)\n",
    "\"\"\"\n",
    "\n",
    "import abc\n",
    "from collections import deque\n",
    "import functools\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import operator\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.092974268256818\n",
      "10.0\n",
      "-0.34287351182006054\n",
      "3.8185948536513634\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Core of the framework, variables and functions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Variable:\n",
    "    def __init__(self, value: float, from_op=None, requires_grad=False):\n",
    "        self.value = float(value)\n",
    "        self.gradient = None\n",
    "        self.from_op = from_op\n",
    "        self.requires_grad = requires_grad\n",
    "        self.gradient_fcts = []\n",
    "    \n",
    "    def compute_gradient(self):\n",
    "        if self.requires_grad:\n",
    "            self.gradient = sum(fct.derivative_by(self) for fct in self.gradient_fcts) if self.gradient_fcts else 1\n",
    "    \n",
    "    def backward(self):\n",
    "        to_visit = deque([self])\n",
    "        while to_visit:\n",
    "            node = to_visit.popleft()\n",
    "            if node.requires_grad:\n",
    "                node.compute_gradient()\n",
    "                if node.from_op:\n",
    "                    to_visit.extend(arg for arg in node.from_op.arguments)\n",
    "\n",
    "    \n",
    "class Function(abc.ABC):\n",
    "    # TODO - make it a metaclass\n",
    "    # TODO - you could make it a Monad in Haskell\n",
    "    \n",
    "    def __init__(self, arguments: List[Variable]):\n",
    "        self.arguments = arguments\n",
    "        self.output = None\n",
    "    \n",
    "    def __call__(self) -> Variable:\n",
    "        result = self.apply(arg.value for arg in self.arguments)\n",
    "        self.output = Variable(result, self, requires_grad=False)\n",
    "        for arg in self.arguments:\n",
    "            if arg.requires_grad:\n",
    "                arg.gradient_fcts.append(self)\n",
    "                self.output.requires_grad=True\n",
    "        return self.output\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def apply(self, argument_values) -> Variable:\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def derivative_by(self, by: Variable) -> float:\n",
    "        pass\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Example of functions\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "class AddFct(Function):\n",
    "    def __init__(self, arguments):\n",
    "        super().__init__(arguments)\n",
    "    \n",
    "    def apply(self, argument_values):\n",
    "        return functools.reduce(operator.add, argument_values, 0)\n",
    "    \n",
    "    def derivative_by(self, x: Variable):\n",
    "        if x in self.arguments:\n",
    "            return self.output.gradient\n",
    "        return 0\n",
    "\n",
    "\n",
    "class MultiplyFct(Function):\n",
    "    def __init__(self, arguments):\n",
    "        super().__init__(arguments)\n",
    "    \n",
    "    def apply(self, argument_values):\n",
    "        return functools.reduce(operator.mul, argument_values, 1)\n",
    "    \n",
    "    def derivative_by(self, x: Variable):\n",
    "        total = 1.\n",
    "        for arg in self.arguments:\n",
    "            if arg is not x:\n",
    "                total *= arg.value\n",
    "        return total * self.output.gradient\n",
    "\n",
    "    \n",
    "class SinusFct(Function):\n",
    "    def __init__(self, arguments):\n",
    "        super().__init__(arguments)\n",
    "    \n",
    "    def apply(self, argument_values):\n",
    "        return math.sin(self.arguments[0].value)\n",
    "    \n",
    "    def derivative_by(self, x: Variable):\n",
    "        if x is self.arguments[0]:\n",
    "            return math.cos(self.arguments[0].value) * self.output.gradient\n",
    "        return 0\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Helper functions to create arbitrary expressions\n",
    "\"\"\"\n",
    "    \n",
    "def var(value):\n",
    "    return Variable(value, requires_grad=True)\n",
    "    \n",
    "def constant(value):\n",
    "    return Variable(value, requires_grad=False)\n",
    "    \n",
    "def add(v1: Variable, v2: Variable):\n",
    "    op = AddFct([v1, v2])\n",
    "    return op()\n",
    "\n",
    "def multiply(v1: Variable, v2: Variable):\n",
    "    op = MultiplyFct([v1, v2])\n",
    "    return op()\n",
    "\n",
    "def sinus(v: Variable):\n",
    "    op = SinusFct([v])\n",
    "    return op()\n",
    "\n",
    "def cosinus(v: Variable):\n",
    "    shifted = add(v, constant(math.pi))\n",
    "    op = SinusFct([shifted])\n",
    "    return op()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Examples\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def test():\n",
    "    x1 = var(1)\n",
    "    x2 = var(2)\n",
    "    x3 = var(3)\n",
    "    y1 = add(x1, sinus(x2))\n",
    "    y2 = add(x2, x3)\n",
    "    z = multiply(y1, y2)\n",
    "    l = multiply(constant(2), z)\n",
    "    l.backward()\n",
    "\n",
    "    print(l.value)\n",
    "    print(x1.gradient)\n",
    "    print(x2.gradient)\n",
    "    print(x3.gradient)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19.0930], grad_fn=<MulBackward>)\n",
      "tensor([10.])\n",
      "tensor([-0.3429])\n",
      "tensor([3.8186])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def expectations():\n",
    "    x1 = torch.tensor([1.], requires_grad=True)\n",
    "    x2 = torch.tensor([2.], requires_grad=True)\n",
    "    x3 = torch.tensor([3.], requires_grad=True)\n",
    "\n",
    "    y1 = x1 + torch.sin(x2)\n",
    "    y2 = x2 + x3\n",
    "    z = y1 * y2\n",
    "    l = 2 * z\n",
    "    \n",
    "    l.backward()\n",
    "    \n",
    "    print(l)\n",
    "    print(x1.grad)\n",
    "    print(x2.grad)\n",
    "    print(x3.grad)\n",
    "\n",
    "expectations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The basis for a framework of automatic differentiation such as PyTorch.\n",
    "\n",
    "What we want is to:\n",
    "1. Build a forward pass manually, passing tensors / variable through blocks\n",
    "2. Make sure each variable remember which block uses it (to get its gradients)\n",
    "\"\"\"\n",
    "\n",
    "import abc\n",
    "from collections import deque\n",
    "import functools\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import operator\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "8.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "class Variable:\n",
    "    def __init__(self, value: float, from_op=None, requires_grad=False):\n",
    "        self.value = float(value)\n",
    "        self.gradient = None\n",
    "        self.from_op = from_op\n",
    "        self.requires_grad = requires_grad\n",
    "        self.gradient_fcts = []\n",
    "    \n",
    "    def compute_gradient(self):\n",
    "        if self.requires_grad:\n",
    "            self.gradient = sum(fct.derivative_by(self) for fct in self.gradient_fcts) if self.gradient_fcts else 1\n",
    "    \n",
    "    def backward(self):\n",
    "        to_visit = deque([self])\n",
    "        while to_visit:\n",
    "            node = to_visit.popleft()\n",
    "            if node.requires_grad:\n",
    "                node.compute_gradient()\n",
    "                if node.from_op:\n",
    "                    to_visit.extend(arg for arg in node.from_op.arguments)\n",
    "\n",
    "    \n",
    "class Function(abc.ABC):\n",
    "    # TODO - make it a metaclass\n",
    "    # TODO - you could make it a Monad in Haskell\n",
    "    \n",
    "    def __init__(self, arguments: List[Variable]):\n",
    "        self.arguments = arguments\n",
    "        self.output = None\n",
    "    \n",
    "    def __call__(self) -> Variable:\n",
    "        result = self.apply(arg.value for arg in self.arguments)\n",
    "        self.output = Variable(result, self, requires_grad=False)\n",
    "        for arg in self.arguments:\n",
    "            if arg.requires_grad:\n",
    "                arg.gradient_fcts.append(self)\n",
    "                self.output.requires_grad=True\n",
    "        return self.output\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def apply(self, argument_values) -> Variable:\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def derivative_by(self, by: Variable) -> float:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class AddFct(Function):\n",
    "    def __init__(self, arguments):\n",
    "        super().__init__(arguments)\n",
    "    \n",
    "    def apply(self, argument_values):\n",
    "        return functools.reduce(operator.add, argument_values, 0)\n",
    "    \n",
    "    def derivative_by(self, x: Variable):\n",
    "        if x in self.arguments:\n",
    "            return self.output.gradient\n",
    "        return 0\n",
    "\n",
    "\n",
    "class MultiplyFct(Function):\n",
    "    def __init__(self, arguments):\n",
    "        super().__init__(arguments)\n",
    "    \n",
    "    def apply(self, argument_values):\n",
    "        return functools.reduce(operator.mul, argument_values, 1)\n",
    "    \n",
    "    def derivative_by(self, x: Variable):\n",
    "        total = 1.\n",
    "        for arg in self.arguments:\n",
    "            if arg != x:\n",
    "                total *= arg.value\n",
    "        return total\n",
    "    \n",
    "    \n",
    "def add(v1: Variable, v2: Variable):\n",
    "    op = AddFct([v1, v2])\n",
    "    return op()\n",
    "\n",
    "\n",
    "def multiply(v1: Variable, v2: Variable):\n",
    "    op = MultiplyFct([v1, v2])\n",
    "    return op()\n",
    "\n",
    "\n",
    "x1 = Variable(1, requires_grad=True)\n",
    "x2 = Variable(2, requires_grad=True)\n",
    "x3 = Variable(3, requires_grad=True)\n",
    "y1 = add(x1, x2)\n",
    "y2 = add(x2, x3)\n",
    "z = multiply(y1, y2)\n",
    "z.backward()\n",
    "\n",
    "print(x1.gradient)\n",
    "print(x2.gradient)\n",
    "print(x3.gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

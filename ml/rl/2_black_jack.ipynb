{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Solving the Black Jack via reinforcement learning to find the optimal strategy:\n",
    "- via Dynamic Programming (RL)\n",
    "- via Monte Carlo methods (on-policy and off-policy)\n",
    "\n",
    "Rules of the Black Jack here (one vs one against the dealer):\n",
    "- if sum of the card goes above 21, you bust (lose), else you can stick or hit\n",
    "- dealer has deterministic way of playing: sticks if higher than 17, else hits\n",
    "- the ace can count as either 1 or 11, and you start with 2 cards\n",
    "- you see one of the card of the dealer\n",
    "- the number of cards is INFINITE\n",
    "\"\"\"\n",
    "\n",
    "from collections import *\n",
    "from dataclasses import *\n",
    "import enum\n",
    "import numpy as np\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of the game of Blackjack\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Action(enum.Enum):\n",
    "    STICK = 0\n",
    "    HIT = 1\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class VisibleState:\n",
    "    dealer_card: int\n",
    "    current_total: int\n",
    "    has_usable_ace: bool\n",
    "\n",
    "        \n",
    "class Hand:\n",
    "    DECK = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10]\n",
    "    \n",
    "    def __init__(self, cards):\n",
    "        self.cards = list(cards)\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls):\n",
    "        return cls(cards=np.random.choice(cls.DECK, size=2))\n",
    "    \n",
    "    def pick_card(self):\n",
    "        self.cards.append(np.random.choice(self.DECK))\n",
    "    \n",
    "    @property\n",
    "    def first_card(self):\n",
    "        return self.cards[0]\n",
    "    \n",
    "    @property\n",
    "    def total(self) -> int:\n",
    "        return self.state[0]\n",
    "    \n",
    "    @property\n",
    "    def state(self) -> Tuple[int, bool]:\n",
    "        total = 0\n",
    "        usable_ace = 0\n",
    "        for card in self.cards:\n",
    "            if card == 1:\n",
    "                total += 11\n",
    "                usable_ace += 1\n",
    "            else:\n",
    "                total += card\n",
    "            if total > 21 and usable_ace > 0:\n",
    "                total -= 10\n",
    "                usable_ace -= 1\n",
    "        return total, usable_ace\n",
    "\n",
    "    \n",
    "Reward = int\n",
    "\n",
    "\n",
    "class BlackJack:\n",
    "    def __init__(self):\n",
    "        self.dealer = None\n",
    "        self.player = None\n",
    "        self.is_over = False\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.dealer = Hand.random()\n",
    "        self.player = Hand.random()\n",
    "        self.is_over = False\n",
    "    \n",
    "    def get_state(self) -> VisibleState:\n",
    "        total, usable_ace = self.player.state\n",
    "        return VisibleState(\n",
    "            dealer_card = self.dealer.first_card,\n",
    "            current_total = total,\n",
    "            has_usable_ace = usable_ace > 0)\n",
    "    \n",
    "    def get_actions(self) -> List[Action]:\n",
    "        return [Action.STICK, Action.HIT]\n",
    "    \n",
    "    def play(self, action) -> Reward:\n",
    "        if action == Action.HIT:\n",
    "            self.player.pick_card()\n",
    "            self._dealer_move()\n",
    "            if self.player.total > 21:\n",
    "                self.is_over = True\n",
    "                return -1\n",
    "            elif self.dealer.total > 21:\n",
    "                self.is_over = True\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        elif action == Action.STICK:\n",
    "            self.is_over = True\n",
    "            if self.player.total > self.dealer.total:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return -1 # Invalid action: you loose\n",
    "    \n",
    "    def _dealer_move(self):\n",
    "        if self.dealer.total < 17:\n",
    "            self.dealer.pick_card()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classical Policity Iteration:\n",
    "- first do some rounds of \"Policy Evaluation\": improve the state value V (or action value Q) under the current policy P\n",
    "- then adapt the policy P to become greedy regarding the state value V (or action value Q)\n",
    "- keep on doing this while there are some changes in the policy P (or enough change in V or Q)\n",
    "\n",
    "There are two ways to try to perform the Policy Valuation:\n",
    "- via Dymanic Programming: you open all possibilities and just look at the next step (requires to know the dynamic of the game)\n",
    "- via Monte Carlo (either on-policy or off-policy): you generate some games to find the state value V (or action value Q)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dynamic Programming way of doing the \"Policy Evaluation\":\n",
    "\n",
    "The \"Bellman Update\" is used for the Policy Evaluation:\n",
    "- unroll the equation of the state value V (or action value Q), to turn it into an update rule\n",
    "- by introducing \"time\" in order to use the same notion as in typical Dynamic Programming)\n",
    "\n",
    "V(s)   = Expected[a ~ policy] { Q(s, a) }\n",
    "Q(s,a) = Expected[(r,s') ~ p] {r + gamma * V(s')} \n",
    "\n",
    "Becomes (for state evaluation - but the action evaluation is not especially useful as the model is fully known in DP):\n",
    "\n",
    "V(s) = Expected[a ~ policy] { Expected[(r,s') ~ p] {r + gamma * V(s')} }\n",
    "\n",
    "The Policy Evaluation algorithm itself becomes:\n",
    "\n",
    "    initialize all V(s) arbitrarily (except terminal states to 0)\n",
    "    max_diff = 0.\n",
    "    while max_diff < epsilon:\n",
    "        max_diff = 0.\n",
    "        for each state s:\n",
    "            previous_v = V(s)\n",
    "            V(s) = Expected[a ~ policy] { Expected[(r,s') ~ p] {r + gamma * V(s')} }\n",
    "            max_diff = max(max_diff, abs(previous_v - V(s)))\n",
    "    return all V(s)\n",
    "\"\"\"\n",
    "\n",
    "# TODO - but this really hard, since you need to know the distribution of probability p for (s', r) given (s, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Monte Carlo of doing the \"Policy Evaluation\" following an \"on-policy\" approach:\n",
    "- we use the policy to generate new scenarios\n",
    "- and we keep track of the states and reward to estimate the value\n",
    "\n",
    "Here we evaluate the state value V, although in the end, this is not very useful:\n",
    "- if we do not know the model, we cannot really use it to take a decision (can't look ahead to next value)\n",
    "- in such cases, it is either useful to try to evaluate the probability p(s',r|s,a) of the model or to search Q instead of V\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def every_visit_monte_carlo_policy_state_evaluation(game: BlackJack, gamma: float, nb_episodes: int, policy):\n",
    "    state_values: Dict[VisibleState, float] = defaultdict(float)\n",
    "    state_counts: Dict[VisibleState, int] = defaultdict(int)\n",
    "    \n",
    "    for _ in range(nb_episodes):\n",
    "        game.reset()\n",
    "        \n",
    "        states = []\n",
    "        rewards = []\n",
    "        while not game.is_over:\n",
    "            state = game.get_state()\n",
    "            action = policy(state)\n",
    "            reward = game.play(action)\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        score = 0.\n",
    "        for i in reversed(range(len(states))):\n",
    "            state = states[i]\n",
    "            reward = rewards[i]\n",
    "            score = reward + gamma * score\n",
    "            state_values[state] += 1 / (state_counts[state] + 1) * (score - state_values[state])\n",
    "            state_counts[state] += 1\n",
    "    \n",
    "    return state_values\n",
    "\n",
    "\n",
    "def initial_policy(state: VisibleState):\n",
    "    if state.current_total < 20:\n",
    "        return Action.HIT\n",
    "    else:\n",
    "        return Action.STICK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n"
     ]
    }
   ],
   "source": [
    "state_values = every_visit_monte_carlo_policy_state_evaluation(\n",
    "    game=BlackJack(),\n",
    "    gamma=1.,\n",
    "    nb_episodes=1000,\n",
    "    policy=initial_policy)\n",
    "\n",
    "print(len(state_values)) # Should be up to 10 (dealer card) * (21-2) (total for player) * 2 (ace or not ace) = 380"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If we search to evaluate Q instead of V, we face the problem of having to try all possibles (s,a):\n",
    "- either we need to try every possible (s, a) as starting action\n",
    "- or we need to introduce some randomness in our policy (epsilon greedy policy) else it will only select the same action\n",
    "\n",
    "If we introduce some randomness, we must realize we do not solve the original problem, but solve a problem of finding the optimal\n",
    "policy in an environment that does not not really select our chosen action all the time.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "StateAction = Tuple[VisibleState, Action]\n",
    "\n",
    "\n",
    "def with_epsilon_random_action(actions: List[Action], epsilon: float, policy):\n",
    "    def epsilon_policy(state: VisibleState):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            return np.random.choice(actions)\n",
    "        else:\n",
    "            return policy(state)\n",
    "    return epsilon_policy\n",
    "\n",
    "\n",
    "def every_visit_monte_carlo_policy_action_evaluation(game: BlackJack, gamma: float, nb_episodes: int, policy):\n",
    "    action_values: Dict[StateAction, float] = defaultdict(float)\n",
    "    action_counts: Dict[StateAction, int] = defaultdict(int)\n",
    "    \n",
    "    for _ in range(nb_episodes):\n",
    "        game.reset()\n",
    "        \n",
    "        transitions = []\n",
    "        rewards = []\n",
    "        while not game.is_over:\n",
    "            state = game.get_state()\n",
    "            action = policy(state)\n",
    "            reward = game.play(action)\n",
    "            transitions.append((state, action))\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        score = 0.\n",
    "        for i in reversed(range(len(transitions))):\n",
    "            state, action = transitions[i]\n",
    "            reward = rewards[i]\n",
    "            score = reward + gamma * score\n",
    "            action_values[(state, action)] += 1 / (action_counts[(state, action)] + 1) * (score - action_values[(state, action)])\n",
    "            action_counts[(state, action)] += 1\n",
    "    \n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n"
     ]
    }
   ],
   "source": [
    "action_values = every_visit_monte_carlo_policy_action_evaluation(\n",
    "    game=BlackJack(),\n",
    "    gamma=1.,\n",
    "    nb_episodes=10_000,\n",
    "    policy=with_epsilon_random_action([Action.HIT, Action.STICK], 0.1, initial_policy))\n",
    "\n",
    "print(len(action_values)) # Should be up to 10 (dealer card) * (21-2) (total for player) * 2 (ace or not ace) * 2 (actions) = 760"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

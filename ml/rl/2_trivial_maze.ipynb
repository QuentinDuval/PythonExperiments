{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic movement inside a maze\n",
    "\"\"\"\n",
    "\n",
    "# TODO - FOR FUTURE GAME\n",
    "# Rewards should be such you get -1 while you are inside the maze.\n",
    "# Getting a reward at the end does not convey the same information:\n",
    "# - whether or not you find the end, you get the same result: YOU WANT TO FIND IT ASAP\n",
    "# - with discounting future rewards, it might actually makes the strategy of staying a valid one\n",
    "\n",
    "from collections import *\n",
    "from dataclasses import dataclass\n",
    "import enum\n",
    "import numpy as np\n",
    "from typing import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A game in which you are supposed to find the bottom-right corner of the map\n",
    "- There are positions in which you gain some power pills\n",
    "- There are positions in which you just loose\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "Position = Tuple[int, int]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class State:\n",
    "    position: Position\n",
    "    # territory: np.ndarray\n",
    "\n",
    "\n",
    "class Move(enum.Enum):\n",
    "    DOWN = 0\n",
    "    RIGHT = 1\n",
    "\n",
    "\n",
    "class FindYourWayEnv:\n",
    "    \n",
    "    @classmethod\n",
    "    def create_random(cls, height: int, width: int, bombs: int, coins: int):\n",
    "        territory = np.zeros((height, width))\n",
    "        coordinates = [(i, j) for i in range(height) for j in range(width) if (i, j) != (height-1, width-1) and (i, j) != (0, 0)]\n",
    "        np.random.shuffle(coordinates)\n",
    "        for count, (i, j) in enumerate(coordinates[:bombs+coins]):\n",
    "            if count < bombs:\n",
    "                territory[i][j] = -50.\n",
    "            else:\n",
    "                territory[i][j] = 1\n",
    "        territory[height-1][width-1] = 100\n",
    "        return cls(territory)\n",
    "    \n",
    "    def __init__(self, territory):\n",
    "        self.territory = territory\n",
    "        self.h = territory.shape[0]\n",
    "        self.w = territory.shape[1]\n",
    "        self.i = 0\n",
    "        self.j = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.i = 0\n",
    "        self.j = 0\n",
    "\n",
    "    def sample(self):\n",
    "        self.i = np.random.randint(0, self.h-1)\n",
    "        self.j = np.random.randint(0, self.w-1)\n",
    "\n",
    "    def get_state(self) -> State:\n",
    "        return State(position=(self.i, self.j))\n",
    "\n",
    "    def get_actions(self) -> List[Move]:\n",
    "        actions = []\n",
    "        if self.i < self.h-1:\n",
    "            actions.append(Move.DOWN)\n",
    "        if self.j < self.w-1:\n",
    "            actions.append(Move.RIGHT)\n",
    "        return actions\n",
    "\n",
    "    def is_done(self) -> bool:\n",
    "        return self.i == self.h-1 and self.j == self.w-1\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.is_done():\n",
    "            raise Exception(\"Game is over\")\n",
    "        if action == Move.DOWN:\n",
    "            self.i += 1\n",
    "        elif action == Move.RIGHT:\n",
    "            self.j += 1\n",
    "        reward = self.territory[self.i][self.j]\n",
    "        return reward - 1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Same game, but the agent can slip to the bottom\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SlipperyFindYourWayEnv(FindYourWayEnv):\n",
    "    def __init__(self, territory, slip_prob=0.2):\n",
    "        super().__init__(territory)\n",
    "        self.slip_prob = slip_prob\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0 and 1 in self.get_actions():\n",
    "            if random.random() < self.slip_prob:\n",
    "                action = 1\n",
    "        return super().step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An agent that uses Q-learning (in fact, a variant called Tabular Q-learning)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self):\n",
    "        self.actions = defaultdict(set)             # map state to possible actions at that state\n",
    "        self.transitions = defaultdict(Counter)     # map tuple (state, action) to expected target states (with prob)\n",
    "        self.rewards = defaultdict(float)           # map tuple (state, action, new_state) to reward\n",
    "        self.q_values = defaultdict(float)          # map tuple (state, action) to expected value\n",
    "        self.temperature = 1.                       # controls the number of random actions attempted\n",
    "        self.discount = 0.9                         # discount factor used in Q-learning bellman update\n",
    "        self.blending = 0.2\n",
    "\n",
    "    def step(self, env) -> float:\n",
    "        state = env.get_state()\n",
    "        actions = env.get_actions()\n",
    "        if state not in self.actions:\n",
    "            self.actions[state] = set(actions)\n",
    "        action = self._select_action(state, actions)\n",
    "        reward = env.step(action)\n",
    "        self._value_iteration(state, action, env.get_state(), reward)\n",
    "        return reward\n",
    "\n",
    "    def _select_action(self, state, actions):\n",
    "        \"\"\"\n",
    "        Select the next action:\n",
    "        - with a small chance, take one at random, or\n",
    "        - pick the one with the best Q-value (expected long term reward)\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.temperature:\n",
    "            return np.random.choice(actions)\n",
    "\n",
    "        best_action = None\n",
    "        best_action_reward = -1 * float('inf')\n",
    "        for action in actions:\n",
    "            reward = self.q_values[(state, action)]\n",
    "            if reward > best_action_reward:\n",
    "                best_action = action\n",
    "                best_action_reward = reward\n",
    "        return best_action\n",
    "\n",
    "    def _value_iteration(self, state, action, new_state, reward):\n",
    "        \"\"\"\n",
    "        Update the Q-value table based on the transition and reward obtained by the action\n",
    "        !! It is not as simple as updating based on target state (the transitions are random) !!\n",
    "        \"\"\"\n",
    "        self.rewards[(state, action, new_state)] = reward\n",
    "        self.transitions[(state, action)][new_state] += 1\n",
    "\n",
    "        expected_reward = 0.\n",
    "        expected_value = 0.\n",
    "        total_transitions = sum(self.transitions[(state, action)].values())\n",
    "        for new_state, count in self.transitions[(state, action)].items():\n",
    "            max_next_value = max((self.q_values[(new_state, action)] for action in self.actions[new_state]), default=0)\n",
    "            expected_reward += (count / total_transitions) * self.rewards[(state, action, new_state)]\n",
    "            expected_value += (count / total_transitions) * max_next_value\n",
    "        self.q_values[(state, action)] = \\\n",
    "            (1 - self.blending) * self.q_values[(state, action)] + \\\n",
    "            self.blending * (expected_reward + self.discount * expected_value)\n",
    "\n",
    "    def temperature_decrease(self, decrease=0.1):\n",
    "        self.temperature -= decrease\n",
    "        self.temperature = max(self.temperature, 0.)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str({'q_values': self.q_values, 'temperature': self.temperature})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 : 50.34000000000002  (temperature 1.0)\n",
      "Episode 200 : 58.000000000000014  (temperature 0.8)\n",
      "Episode 300 : 60.79999999999999  (temperature 0.6000000000000001)\n",
      "Episode 400 : 59.00000000000001  (temperature 0.4000000000000001)\n",
      "Episode 500 : 65.49000000000001  (temperature 0.20000000000000007)\n",
      "Episode 600 : 74.47000000000001  (temperature 5.551115123125783e-17)\n",
      "Episode 700 : 78.10000000000001  (temperature 0.0)\n",
      "Episode 800 : 84.55999999999996  (temperature 0.0)\n",
      "Episode 900 : 86.51000000000002  (temperature 0.0)\n",
      "Episode 1000 : 87.49999999999996  (temperature 0.0)\n",
      "Episode 100 : 55.910000000000025  (temperature 1.0)\n",
      "Episode 200 : 54.59  (temperature 0.8)\n",
      "Episode 300 : 58.68000000000001  (temperature 0.6000000000000001)\n",
      "Episode 400 : 63.07  (temperature 0.4000000000000001)\n",
      "Episode 500 : 64.88000000000001  (temperature 0.20000000000000007)\n",
      "Episode 600 : 73.97  (temperature 5.551115123125783e-17)\n",
      "Episode 700 : 79.06999999999996  (temperature 0.0)\n",
      "Episode 800 : 84.05999999999999  (temperature 0.0)\n",
      "Episode 900 : 87.00999999999999  (temperature 0.0)\n",
      "Episode 1000 : 87.49999999999996  (temperature 0.0)\n"
     ]
    }
   ],
   "source": [
    "def get_bomb_pattern():\n",
    "    return np.array([\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
    "        [0., 0., 1., 1., 1., 1., 1., 0., 0.],\n",
    "        [0., 0., 1., 1., 1., 1., 1., 0., 0.],\n",
    "        [0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_coin_pattern():\n",
    "    return np.array([\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 1., 0., 1., 1., 1., 0., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
    "        [0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]\n",
    "    ])\n",
    "\n",
    "\n",
    "def render(image, bomb_pattern, coin_pattern):\n",
    "    init_h, init_w = image.shape\n",
    "    scale_h, scale_w = bomb_pattern.shape\n",
    "    h = init_h * scale_h\n",
    "    w = init_w * scale_w\n",
    "    magnified = np.zeros((h, w), 'uint8')\n",
    "    for i in range(init_h):\n",
    "        for j in range(init_w):\n",
    "            x = scale_h * i\n",
    "            y = scale_w * j\n",
    "            if image[i][j] > 0:\n",
    "                magnified[x:x+scale_h,y:y+scale_w] = coin_pattern\n",
    "            elif image[i][j] < 0:\n",
    "                magnified[x:x+scale_h,y:y+scale_w] = bomb_pattern\n",
    "    return magnified\n",
    "\n",
    "\n",
    "def show_maze(territory: np.ndarray):\n",
    "    image = render(territory, get_bomb_pattern(), get_coin_pattern()) * 255\n",
    "    image = Image.fromarray(image, 'L')\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def run_episode(env, agent):\n",
    "    total_reward = 0.\n",
    "    env.reset()\n",
    "    while not env.is_done():\n",
    "        total_reward += agent.step(env)\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "class RunningAverage:\n",
    "    def __init__(self):\n",
    "        self.average = 0.\n",
    "        self.count = 0\n",
    "    \n",
    "    def add(self, value):\n",
    "        self.average += 1 / (self.count + 1) * (value - self.average)\n",
    "        self.count += 1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.average = 0.\n",
    "        self.count = 0\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.average\n",
    "\n",
    "\n",
    "def train_q_agent(env, agent):\n",
    "    running_average = RunningAverage()\n",
    "    for episode in range(1, 1001):\n",
    "        reward = run_episode(env, agent)\n",
    "        running_average.add(reward)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(\"Episode\", episode, \":\", running_average(), \" (temperature \" + str(agent.temperature) + \")\")\n",
    "            agent.temperature_decrease(0.2)\n",
    "            running_average.reset()\n",
    "\n",
    "\n",
    "env = FindYourWayEnv.create_random(height=8, width=8, bombs=3, coins=5)\n",
    "slippery_env = SlipperyFindYourWayEnv(env.territory, slip_prob=0.2)\n",
    "show_maze(env.territory)\n",
    "train_q_agent(env=env, agent=QAgent())\n",
    "train_q_agent(env=slippery_env, agent=QAgent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic movement inside a maze\n",
    "\"\"\"\n",
    "\n",
    "# TODO - FOR FUTURE GAME\n",
    "# Rewards should be such you get -1 while you are inside the maze.\n",
    "# Getting a reward at the end does not convey the same information:\n",
    "# - whether or not you find the end, you get the same result: YOU WANT TO FIND IT ASAP\n",
    "# - with discounting future rewards, it might actually makes the strategy of staying a valid one\n",
    "\n",
    "from collections import *\n",
    "from dataclasses import dataclass\n",
    "import enum\n",
    "import numpy as np\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A game in which you are supposed to find the bottom-right corner of the map\n",
    "- There are positions in which you gain some power pills\n",
    "- There are positions in which you just loose\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "Position = Tuple[int, int]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class State:\n",
    "    position: Position\n",
    "    # territory: np.ndarray\n",
    "\n",
    "\n",
    "class Move(enum.Enum):\n",
    "    DOWN = 0\n",
    "    RIGHT = 1\n",
    "\n",
    "\n",
    "class FindYourWayEnv:\n",
    "    \n",
    "    @classmethod\n",
    "    def create_random(cls, height: int, width: int, bombs: int, coins: int):\n",
    "        territory = np.zeros((height, width))\n",
    "        coordinates = [(i, j) for i in range(height) for j in range(width) if (i, j) != (height-1, width-1)]\n",
    "        np.random.shuffle(coordinates)\n",
    "        for count, (i, j) in enumerate(coordinates[:bombs+coins]):\n",
    "            if count <= bombs:\n",
    "                territory[i][j] = -50.\n",
    "            else:\n",
    "                territory[i][j] = 1\n",
    "        territory[height-1][width-1] = 100\n",
    "        return cls(territory)\n",
    "    \n",
    "    def __init__(self, territory):\n",
    "        self.territory = territory\n",
    "        self.h = territory.shape[0]\n",
    "        self.w = territory.shape[1]\n",
    "        self.i = 0\n",
    "        self.j = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.i = 0\n",
    "        self.j = 0\n",
    "\n",
    "    def sample(self):\n",
    "        self.i = np.random.randint(0, self.h-1)\n",
    "        self.j = np.random.randint(0, self.w-1)\n",
    "\n",
    "    def get_state(self) -> State:\n",
    "        return State(position=(self.i, self.j))\n",
    "\n",
    "    def get_actions(self) -> List[Move]:\n",
    "        actions = []\n",
    "        if self.i < self.h-1:\n",
    "            actions.append(Move.DOWN)\n",
    "        if self.j < self.w-1:\n",
    "            actions.append(Move.RIGHT)\n",
    "        return actions\n",
    "\n",
    "    def is_done(self) -> bool:\n",
    "        return self.i == self.h-1 and self.j == self.w-1\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.is_done():\n",
    "            raise Exception(\"Game is over\")\n",
    "        if action == Move.DOWN:\n",
    "            self.i += 1\n",
    "        elif action == Move.RIGHT:\n",
    "            self.j += 1\n",
    "        reward = self.territory[self.i][self.j]\n",
    "        return reward - 1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Same game, but the agent can slip to the bottom\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SlipperyFindYourWayEnv(FindYourWayEnv):\n",
    "    def __init__(self, territory, slip_prob=0.2):\n",
    "        super().__init__(territory)\n",
    "        self.slip_prob = slip_prob\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0 and 1 in self.get_actions():\n",
    "            if random.random() < self.slip_prob:\n",
    "                action = 1\n",
    "        return super().step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An agent that uses Q-learning (in fact, a variant called Tabular Q-learning)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self):\n",
    "        self.actions = defaultdict(set)             # map state to possible actions at that state\n",
    "        self.transitions = defaultdict(Counter)     # map tuple (state, action) to expected target states (with prob)\n",
    "        self.rewards = defaultdict(float)           # map tuple (state, action, new_state) to reward\n",
    "        self.q_values = defaultdict(float)          # map tuple (state, action) to expected value\n",
    "        self.temperature = 1.                       # controls the number of random actions attempted\n",
    "        self.discount = 0.9                         # discount factor used in Q-learning bellman update\n",
    "        self.blending = 0.2\n",
    "\n",
    "    def step(self, env) -> float:\n",
    "        state = env.get_state()\n",
    "        actions = env.get_actions()\n",
    "        if state not in self.actions:\n",
    "            self.actions[state] = set(actions)\n",
    "        action = self._select_action(state, actions)\n",
    "        reward = env.step(action)\n",
    "        self._value_iteration(state, action, env.get_state(), reward)\n",
    "        return reward\n",
    "\n",
    "    def _select_action(self, state, actions):\n",
    "        \"\"\"\n",
    "        Select the next action:\n",
    "        - with a small chance, take one at random, or\n",
    "        - pick the one with the best Q-value (expected long term reward)\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.temperature:\n",
    "            return np.random.choice(actions)\n",
    "\n",
    "        best_action = None\n",
    "        best_action_reward = -1 * float('inf')\n",
    "        for action in actions:\n",
    "            reward = self.q_values[(state, action)]\n",
    "            if reward > best_action_reward:\n",
    "                best_action = action\n",
    "                best_action_reward = reward\n",
    "        return best_action\n",
    "\n",
    "    def _value_iteration(self, state, action, new_state, reward):\n",
    "        \"\"\"\n",
    "        Update the Q-value table based on the transition and reward obtained by the action\n",
    "        !! It is not as simple as updating based on target state (the transitions are random) !!\n",
    "        \"\"\"\n",
    "        self.rewards[(state, action, new_state)] = reward\n",
    "        self.transitions[(state, action)][new_state] += 1\n",
    "\n",
    "        expected_reward = 0.\n",
    "        expected_value = 0.\n",
    "        total_transitions = sum(self.transitions[(state, action)].values())\n",
    "        for new_state, count in self.transitions[(state, action)].items():\n",
    "            max_next_value = max((self.q_values[(new_state, action)] for action in self.actions[new_state]), default=0)\n",
    "            expected_reward += (count / total_transitions) * self.rewards[(state, action, new_state)]\n",
    "            expected_value += (count / total_transitions) * max_next_value\n",
    "        self.q_values[(state, action)] = \\\n",
    "            (1 - self.blending) * self.q_values[(state, action)] + \\\n",
    "            self.blending * (expected_reward + self.discount * expected_value)\n",
    "\n",
    "    def temperature_decrease(self, decrease=0.1):\n",
    "        self.temperature -= decrease\n",
    "        self.temperature = max(self.temperature, 0.)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str({'q_values': self.q_values, 'temperature': self.temperature})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 : 65.9  (temperature 1.0)\n",
      "Epoch 200 : 59.38  (temperature 0.9)\n",
      "Epoch 300 : 66.89  (temperature 0.8)\n",
      "Epoch 400 : 70.0  (temperature 0.7000000000000001)\n",
      "Epoch 500 : 69.55  (temperature 0.6000000000000001)\n",
      "Epoch 600 : 70.09  (temperature 0.5000000000000001)\n",
      "Epoch 700 : 66.4  (temperature 0.40000000000000013)\n",
      "Epoch 800 : 67.02  (temperature 0.30000000000000016)\n",
      "Epoch 900 : 56.36  (temperature 0.20000000000000015)\n",
      "Epoch 1000 : 50.45  (temperature 0.10000000000000014)\n",
      "Epoch 100 : 65.3  (temperature 1.0)\n",
      "Epoch 200 : 64.33  (temperature 0.9)\n",
      "Epoch 300 : 65.93  (temperature 0.8)\n",
      "Epoch 400 : 71.98  (temperature 0.7000000000000001)\n",
      "Epoch 500 : 75.06  (temperature 0.6000000000000001)\n",
      "Epoch 600 : 70.71  (temperature 0.5000000000000001)\n",
      "Epoch 700 : 67.47  (temperature 0.40000000000000013)\n",
      "Epoch 800 : 67.59  (temperature 0.30000000000000016)\n",
      "Epoch 900 : 58.31  (temperature 0.20000000000000015)\n",
      "Epoch 1000 : 48.97  (temperature 0.10000000000000014)\n"
     ]
    }
   ],
   "source": [
    "def train_q_agent(env, agent):\n",
    "    rewards = []\n",
    "    for epoch in range(1000):\n",
    "        # first collect some stats\n",
    "        '''\n",
    "        for _ in range(10):\n",
    "            env.sample()\n",
    "            if not env.is_done():\n",
    "                agent.step(env)\n",
    "        '''\n",
    "\n",
    "        # play a real game\n",
    "        total_reward = 0.\n",
    "        env.reset()\n",
    "        while not env.is_done():\n",
    "            total_reward += agent.step(env)\n",
    "        rewards.append(total_reward)\n",
    "        if (1 + epoch) % 100 == 0:\n",
    "            print(\"Epoch\", epoch + 1, \":\", np.mean(rewards), \" (temperature \" + str(agent.temperature) + \")\")\n",
    "            agent.temperature_decrease(0.2)\n",
    "            rewards.clear()\n",
    "\n",
    "\n",
    "env = FindYourWayEnv.create_random(height=5, width=5, bombs=2, coins=3)\n",
    "slippery_env = SlipperyFindYourWayEnv(env.territory, slip_prob=0.2)\n",
    "train_q_agent(env=env, agent=QAgent())\n",
    "train_q_agent(env=slippery_env, agent=QAgent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

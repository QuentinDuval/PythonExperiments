{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to force the reload of modules\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# In order to make the import of local modules\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import abc\n",
    "from collections import *\n",
    "from dataclasses import dataclass\n",
    "import enum\n",
    "import gym\n",
    "import heapq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from ml.rl.core import *\n",
    "\n",
    "%matplotlib inline\n",
    "import imageio\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation shape: (2,)\n",
      "action space: {0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Showing information about the environment\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with gym.make(\"MountainCar-v0\") as env:\n",
    "    print(\"observation shape:\", env.reset().shape)\n",
    "    print(\"action space:\", {env.action_space.sample() for _ in range(100)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward -200.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Helpers to try out some random agents\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class PolicyAgent(Agent):\n",
    "    def __init__(self, net):\n",
    "        self.net = net\n",
    "        \n",
    "    def get_action(self, state: np.ndarray):\n",
    "        action_values = self.net(torch.FloatTensor(state))\n",
    "        _, i = torch.max(action_values, dim=-1)\n",
    "        return i.item()\n",
    "\n",
    "\n",
    "def try_agent(agent: Agent, show=True):\n",
    "    with gym.make(\"MountainCar-v0\") as env:\n",
    "        try_agent_on(env, agent, show=show)\n",
    "\n",
    "\n",
    "with gym.make(\"MountainCar-v0\") as env:\n",
    "    try_agent_on(env, RandomAgent(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cross Entropy Method:\n",
    "- Start with a random policy\n",
    "- Play N episodes with the current policy\n",
    "- Take the episodes above a reward boundary (typically percentile 70th)\n",
    "- Train on these \"Elite\" episodes (throw away the uninteresting ones)\n",
    "\n",
    "The problem here is that it will never make progress, because we never have episodes\n",
    "with something else than -200 as reward. We could change the way the reward is done\n",
    "but this is not the case here.\n",
    "\n",
    "DOOMED TO FAIL\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMITATION LEARNING\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "pass # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[A2000/2000 (100.00%) - 6.36 it/s352/2000 (67.60%) - 5.00 it/s1469/2000 (73.45%) - 4.99 it/s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Neural net to learn the policy\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class QValueNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(2, 100), nn.ReLU(), nn.Linear(100, 3))\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return self.fc(xs)\n",
    "    \n",
    "    def clone(self):\n",
    "        cloned = QValueNet()\n",
    "        cloned.load_state_dict(self.state_dict())\n",
    "        return cloned\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Q-Learning training loop\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ProgressMonitor:\n",
    "    def __init__(self):\n",
    "        self.writer = SummaryWriter(comment='mountain-car-v0')\n",
    "        self.episode = 0\n",
    "        self.success_count = 0\n",
    "    \n",
    "    def track(self, values: Dict[str, Union[int, float]]):\n",
    "        for name, value in values.items():\n",
    "            self.writer.add_scalar(name, value, self.episode)\n",
    "        self.episode += 1\n",
    "    \n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "\n",
    "\n",
    "def epsilon_greedy_action(env: gym.Env, net: nn.Module, state: torch.FloatTensor,\n",
    "                          epsilon: float) -> Tuple[int, float]:\n",
    "    action_values = net(state)\n",
    "    if np.random.rand(1) < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "        return action, action_values[action].item()\n",
    "    else:\n",
    "        action_value, action = torch.max(action_values, dim=-1)\n",
    "        return action.item(), action_value.item()\n",
    "\n",
    "    \n",
    "def get_next_state_value(target_net: nn.Module, next_state: torch.FloatTensor):\n",
    "    action_values = target_net(next_state)\n",
    "    max_q, _ = torch.max(action_values, dim=-1)\n",
    "    return max_q.item()\n",
    "\n",
    "\n",
    "def sample_minibatch(net: nn.Module, replay_buffer):\n",
    "    observations, action_indices, target_values = replay_buffer.sample(size=8)\n",
    "    current_values = net(observations)\n",
    "    action_indices = action_indices.unsqueeze(dim=-1)\n",
    "    current_values = torch.gather(current_values, dim=-1, index=action_indices).squeeze(dim=-1)\n",
    "    return current_values, target_values\n",
    "\n",
    "\n",
    "def train_q_learning(\n",
    "    env: gym.Env, net: nn.Module,\n",
    "    episodes: int, discount: float,\n",
    "    learning_rate: float, learning_rate_decay: float,\n",
    "    weight_decay: float,\n",
    "    epsilon: float, epsilon_decay: float):\n",
    "    \n",
    "    net.train()\n",
    "    target_net = net.clone()\n",
    "    target_net.eval()\n",
    "        \n",
    "    count_success = 0\n",
    "    furthest_distance = float('-inf')\n",
    "    monitor = ProgressMonitor()\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=learning_rate_decay)\n",
    "    \n",
    "    replay_buffer = PrioritizedReplayBuffer(max_size=1_000)\n",
    "\n",
    "    for episode_nb in prange(episodes):\n",
    "        episode_loss = 0.\n",
    "        episode_reward = 0.\n",
    "        \n",
    "        done = False\n",
    "        state = torch.FloatTensor(env.reset())\n",
    "        while not done:            \n",
    "            action, action_value = epsilon_greedy_action(env, net, state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "            \n",
    "            # Compute the target action value\n",
    "            target_value = reward + 0. if done else discount * get_next_state_value(target_net, next_state)\n",
    "            replay_buffer.add(state, action, prev_value=action_value, value=target_value)\n",
    "            \n",
    "            # Sample a mini-batch from the replay buffer and apply the bellman update\n",
    "            if len(replay_buffer) >= 500:\n",
    "                current_values, target_values = sample_minibatch(net, replay_buffer)\n",
    "                loss = criterion(current_values, target_values)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                episode_loss += loss.item()\n",
    "            \n",
    "            # Moving to next state\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        # Switching the model every now and then\n",
    "        if (episode_nb + 1) % 100 == 0:\n",
    "            target_net = net.clone()\n",
    "        \n",
    "        # Update the hyper-parameters on success\n",
    "        if episode_reward > -200.:\n",
    "            count_success += 1\n",
    "            epsilon *= epsilon_decay\n",
    "            scheduler.step()\n",
    "\n",
    "        # Record history\n",
    "        furthest_distance = max(furthest_distance, state[0])\n",
    "        monitor.track({\n",
    "            'Episode/reward': episode_reward,\n",
    "            'Episode/successes_ratio': count_success / (episode_nb + 1),\n",
    "            'Episode/furthest': furthest_distance,\n",
    "            'Training/loss': episode_loss,\n",
    "            'Training/epsilon': epsilon})\n",
    "    \n",
    "    monitor.close()\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Training\n",
    "\"\"\"\n",
    "\n",
    "with gym.make(\"MountainCar-v0\") as env:\n",
    "    q_net = QValueNet()\n",
    "    train_q_learning(\n",
    "        env, net=q_net, episodes=2000, discount=0.99,\n",
    "        learning_rate=1e-3, learning_rate_decay=0.9, weight_decay=0.0,\n",
    "        epsilon=0.3, epsilon_decay=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward -150.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Trying the Q learning agent\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with gym.make(\"MountainCar-v0\") as env:\n",
    "    try_agent_on(env, PolicyAgent(q_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

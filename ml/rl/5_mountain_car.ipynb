{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to force the reload of modules\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# In order to make the import of local modules\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import abc\n",
    "from collections import *\n",
    "from dataclasses import dataclass\n",
    "import enum\n",
    "import gym\n",
    "import heapq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from ml.rl.core import *\n",
    "\n",
    "%matplotlib inline\n",
    "import imageio\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation shape: (2,)\n",
      "action space: {0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Showing information about the environment\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with gym.make(\"MountainCar-v0\") as env:\n",
    "    print(\"observation shape:\", env.reset().shape)\n",
    "    print(\"action space:\", {env.action_space.sample() for _ in range(100)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_agent(agent: Agent, show=True):\n",
    "    with gym.make(\"MountainCar-v0\") as env:\n",
    "        total_reward = 0.0\n",
    "        obs = env.reset()\n",
    "        if show:\n",
    "            env.render()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.get_action(env, obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if show:\n",
    "                env.render()\n",
    "        if show:\n",
    "            print(\"Total reward {0:.2f}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward -200.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Random agent\n",
    "\"\"\"\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    def get_action(self, env, state):\n",
    "        return env.action_space.sample()\n",
    "\n",
    "\n",
    "try_agent(RandomAgent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cross Entropy Method:\n",
    "- Start with a random policy\n",
    "- Play N episodes with the current policy\n",
    "- Take the episodes above a reward boundary (typically percentile 70th)\n",
    "- Train on these \"Elite\" episodes (throw away the uninteresting ones)\n",
    "\n",
    "The problem here is that it will never make progress, because we never have episodes\n",
    "with something else than -200 as reward. We could change the way the reward is done\n",
    "but this is not the case here.\n",
    "\n",
    "DOOMED TO FAIL\n",
    "\"\"\"\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMITATION LEARNING\n",
    "\"\"\"\n",
    "\n",
    "pass # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[A2000/2000 (100.00%) - 5.61 it/s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "With Deep Q-Learning\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2, 50, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 3, bias=False))\n",
    "\n",
    "    def forward(self, xs):\n",
    "        ys = self.fc(xs)\n",
    "        return ys\n",
    "    \n",
    "    def clone(self):\n",
    "        cloned = PolicyNet()\n",
    "        cloned.load_state_dict(self.state_dict())\n",
    "        return cloned\n",
    "        \n",
    "\n",
    "    \n",
    "class PolicyAgent:\n",
    "    def __init__(self, sarsa_net):\n",
    "        self.sarsa_net = sarsa_net\n",
    "        \n",
    "    def get_action(self, env, state):\n",
    "        action_values = self.sarsa_net(state)\n",
    "        _, i = torch.max(action_values, dim=-1)\n",
    "        return i.item()\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Training\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, max_size: int):\n",
    "        self.fifo = deque(maxlen=max_size)\n",
    "        self.weights = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, observation, action, prev_value, value):\n",
    "        self.fifo.append((observation, action, value))\n",
    "        self.weights.append(abs(value - prev_value))\n",
    "    \n",
    "    def sample(self, size) -> Tuple['observations', 'actions', 'values']:\n",
    "        observations = []\n",
    "        actions = []\n",
    "        values = []\n",
    "        for observation, action, value in random.choices(self.fifo, weights=self.weights, k=size):\n",
    "            observations.append(observation)\n",
    "            actions.append(action)\n",
    "            values.append(value)\n",
    "        return torch.stack(observations), torch.LongTensor(actions), torch.FloatTensor(values)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fifo)\n",
    "    \n",
    "def epsilon_greedy_action(policy: nn.Module, state, epsilon) -> Tuple[int, float, torch.FloatTensor]:\n",
    "    action_values = policy(state)\n",
    "    if np.random.rand(1) < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "        return action, action_values[action], action_values\n",
    "    else:\n",
    "        action_value, action = torch.max(action_values, dim=-1)\n",
    "        return action.item(), action_value, action_values\n",
    "    \n",
    "\n",
    "def train_sarsa_learning(\n",
    "    env, policy: nn.Module, episodes: int, discount: float,\n",
    "    learning_rate: float, weight_decay: float,\n",
    "    epsilon: float, epsilon_decay: float):\n",
    "    \n",
    "    policy.train()\n",
    "    \n",
    "    count_success = 0\n",
    "    furthest_distance = float('-inf')\n",
    "    writer = SummaryWriter(comment='mountain-car-v0')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(policy.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    replay_buffer = PrioritizedReplayBuffer(max_size=1_000)\n",
    "\n",
    "    frozen_policy = policy.clone()\n",
    "    frozen_policy.eval()\n",
    "\n",
    "    for episode in prange(episodes):\n",
    "        episode_loss = 0.\n",
    "        episode_reward = 0.\n",
    "        episode_min_x = float('inf')\n",
    "        episode_max_x = float('-inf')\n",
    "        \n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        state = torch.FloatTensor(state)\n",
    "        while not done:\n",
    "            episode_min_x = min(episode_min_x, state[0])\n",
    "            episode_max_x = max(episode_max_x, state[0])\n",
    "            \n",
    "            action, action_value, _ = epsilon_greedy_action(policy, state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "            \n",
    "            # Compute the target values\n",
    "            if not done:\n",
    "                next_action_values = frozen_policy(next_state)\n",
    "                max_q, _ = torch.max(next_action_values, dim=-1)\n",
    "                max_q = min(max(-200, max_q.item()), 0) # Clamp to avoid explosion?\n",
    "                target_value = reward + discount * max_q\n",
    "            else:\n",
    "                target_value = reward\n",
    "            replay_buffer.add(state, action, action_value.item(), target_value)\n",
    "            \n",
    "            # Sample a mini-batch from the replay buffer\n",
    "            if len(replay_buffer) >= 500:\n",
    "                observations, action_indices, target_values = replay_buffer.sample(size=8)\n",
    "                current_values = policy(observations)\n",
    "                action_indices = action_indices.unsqueeze(dim=-1)\n",
    "                current_values = torch.gather(current_values, dim=-1, index=action_indices).squeeze(dim=-1)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(current_values, target_values)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                episode_loss += loss.item()\n",
    "            \n",
    "            # Moving to next state\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        # Switching the model every now and then\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            frozen_policy = policy.clone()\n",
    "            frozen_policy.eval()\n",
    "        \n",
    "        # Criteria of success\n",
    "        if episode_reward > -200.:\n",
    "            count_success += 1\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        # Record history\n",
    "        distance = state[0]\n",
    "        furthest_distance = max(furthest_distance, state[0])\n",
    "        writer.add_scalar('Episode/reward', episode_reward, episode)\n",
    "        writer.add_scalar('Episode/successes_ratio', count_success / (episode + 1), episode)\n",
    "        writer.add_scalar('Episode/furthest', furthest_distance, episode)\n",
    "        writer.add_scalar('Episode/distance_lo', episode_min_x, episode)\n",
    "        writer.add_scalar('Episode/distance_hi', episode_max_x, episode)\n",
    "        writer.add_scalar('Training/loss', episode_loss, episode)\n",
    "        writer.add_scalar('Training/epsilon', epsilon, episode)\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Training\n",
    "\"\"\"\n",
    "\n",
    "q_net = PolicyNet()\n",
    "with gym.make(\"MountainCar-v0\") as env:\n",
    "    train_sarsa_learning(\n",
    "        env, policy=q_net, episodes=2000, discount=0.99,\n",
    "        learning_rate=1e-1, weight_decay=0.0,\n",
    "        epsilon=0.3, epsilon_decay=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

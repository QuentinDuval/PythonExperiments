{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from collections import *\n",
    "from dataclasses import dataclass\n",
    "import enum\n",
    "import gym\n",
    "import numpy as np\n",
    "from typing import *\n",
    "\n",
    "%matplotlib inline\n",
    "import imageio\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pyglet\n",
    "from pyglet.gl import *\n",
    "\n",
    "window = pyglet.window.Window(width=300, height=200, display=None)\n",
    "window.clear()\n",
    "\n",
    "from gym.envs.classic_control import rendering\n",
    "viewer = rendering.Viewer(screen_width, screen_height)\n",
    "'''\n",
    "\n",
    "# TODO - complexe... first without the maze, find the place to go\n",
    "\n",
    "# TODO - move through a maze where 1 are blocked, 0 are free, and you must find the end\n",
    "# TODO - use convolution net to find the right decision\n",
    "# TODO - reward is -1 for each time your are in the maze\n",
    "\n",
    "# TODO - make a custom space for actions / states?\n",
    "\n",
    "\n",
    "class MazeEnv(gym.Env):\n",
    "    def __init__(self, maze: np.ndarray, start_pos: Tuple[int, int]):\n",
    "        self.maze: np.ndarray = np.array(maze)\n",
    "        self.start_pos: Tuple[int, int] = start_pos\n",
    "        self.end_positions: Set[Tuple[int, int]] = self._find_end_positions(maze)\n",
    "        self.state = None # Must be immutable? I guess so\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space = gym.spaces.MultiDiscrete(list(maze.shape))\n",
    "    \n",
    "    @property\n",
    "    def done(self) -> bool:\n",
    "        return self.state in self.end_positions\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = self.start_pos\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            raise Exception(\"Game is over\")\n",
    "        self._move(action)\n",
    "        return self.state, -1, self.done, {}\n",
    "\n",
    "    def _find_end_positions(self, maze):\n",
    "        end_positions = set()\n",
    "        h, w = self.maze.shape\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                if self.maze[i, j] == 2:\n",
    "                    end_positions.add((i, j))\n",
    "                    self.maze[i, j] = 0\n",
    "        return end_positions\n",
    "    \n",
    "    def _move(self, action):\n",
    "        i, j = self.state\n",
    "        h, w = self.maze.shape\n",
    "        if action == 0:   # UP\n",
    "            i = max(0, i - 1)\n",
    "        elif action == 1: # DOWN\n",
    "            i = min(h - 1, i + 1)\n",
    "        elif action == 2: # LEFT\n",
    "            j = max(0, j - 1)\n",
    "        elif action == 3: # RIGHT\n",
    "            j = min(w - 1, j + 1)\n",
    "        if self.maze[i, j] == 0:\n",
    "            self.state = (i, j)\n",
    "    \n",
    "    def render_state(self, zoom: int):\n",
    "        h, w = self.maze.shape\n",
    "        m = np.zeros((h, w, 3), 'uint8')\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                if self.maze[i, j] == 1:\n",
    "                    m[i, j, 0] = 255\n",
    "        for i, j in self.end_positions:\n",
    "            m[i, j, 1] = 255\n",
    "        i, j = self.state\n",
    "        m[i, j, 2] = 255\n",
    "        image = Image.fromarray(m, 'RGB')\n",
    "        image = image.resize((w * zoom, h * zoom))\n",
    "        return image        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionValues(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def get_actions(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def get_action_value(self, state, action) -> float:\n",
    "        pass\n",
    "    \n",
    "    def get_best_action(self, state):\n",
    "        best_action = None\n",
    "        best_score = float('-inf')\n",
    "        for action in self.get_actions(state):\n",
    "            score = self.get_action_value(state, action)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "\n",
    "class DiscreteActionValues(ActionValues):\n",
    "    def __init__(self, learning_rate: float = 0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.values = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    def add(self, state, action, score: float) -> float:\n",
    "        self.values[state][action] += self.learning_rate * (score - self.values[state][action])\n",
    "    \n",
    "    def get_actions(self, state):\n",
    "        return self.values[state].keys()\n",
    "    \n",
    "    def get_action_value(self, state, action) -> float:\n",
    "        return self.values[state][action]\n",
    "\n",
    "\n",
    "class SARSA:\n",
    "    def __init__(self,\n",
    "                 default_value: float = 0.,\n",
    "                 learning_rate: float = 0.1,\n",
    "                 reward_discount: float = 1.,\n",
    "                 epsilon: float = 0.1\n",
    "                ):\n",
    "        # a big default_value might help favoring exploration at early stages (but biase the results)\n",
    "        self.q_values = DiscreteActionValues(learning_rate)\n",
    "        self.reward_discount = reward_discount  # discount factor of future reward taken into account at present time\n",
    "        self.epsilon = epsilon                  # probability to take a random action\n",
    "    \n",
    "    def play_episode(self, env) -> float:\n",
    "        total_reward = 0.\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self._behavior_policy_action(env, state)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            new_action = self._target_policy_action(env, new_state)\n",
    "            score = reward + self.reward_discount * self.q_values.get_action_value(new_state, new_action)\n",
    "            self.q_values.add(state, action, score)\n",
    "            total_reward += reward\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "    \n",
    "    def get_action(self, env, state):\n",
    "        return self._target_policy_action(env, state)\n",
    "    \n",
    "    def _target_policy_action(self, env, state):\n",
    "        return self._behavior_policy_action(env, state)\n",
    "    \n",
    "    def _behavior_policy_action(self, env, state):\n",
    "        if self.epsilon > 0. and np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        return self.q_values.get_best_action(state)\n",
    "\n",
    "\n",
    "class QLearning(SARSA):\n",
    "    def __init__(self,\n",
    "                 default_value: float = 0.,\n",
    "                 learning_rate: float = 0.1,\n",
    "                 reward_discount: float = 1.,\n",
    "                 epsilon: float = 0.1\n",
    "                ):\n",
    "        super().__init__(default_value=default_value, learning_rate=learning_rate, reward_discount=reward_discount, epsilon=epsilon)\n",
    "    \n",
    "    def _target_policy_action(self, env, state):\n",
    "        return self.q_values.get_best_action(state)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Training Loop\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class RunningAverage:\n",
    "    def __init__(self):\n",
    "        self.average = 0.\n",
    "        self.count = 0\n",
    "    \n",
    "    def add(self, value):\n",
    "        self.average += 1 / (self.count + 1) * (value - self.average)\n",
    "        self.count += 1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.average = 0.\n",
    "        self.count = 0\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.average\n",
    "\n",
    "\n",
    "def train_agent(env, agent, nb_episodes: int):\n",
    "    running_average = RunningAverage()\n",
    "    temperature_decrease_period = nb_episodes // 21\n",
    "    temperature_decrease = agent.epsilon / 20\n",
    "    for episode in range(1, nb_episodes + 1):\n",
    "        reward = agent.play_episode(env)\n",
    "        running_average.add(reward)\n",
    "        if episode % temperature_decrease_period == 0:\n",
    "            print(\"Episode\", episode, \":\", running_average(), \" (epsilon \" + str(agent.epsilon) + \")\")\n",
    "            agent.epsilon -= temperature_decrease\n",
    "            running_average.reset()\n",
    "    return agent\n",
    "\n",
    "\n",
    "def demo_agent(env, agent, gif_name: str):\n",
    "    images = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(env, state)\n",
    "        images.append(env.render_state(10))\n",
    "        state, reward, done, info = env.step(action)\n",
    "    images.append(env.render_state(10))\n",
    "    imageio.mimsave(gif_name, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23 : -905.4347826086955  (epsilon 0.1)\n",
      "Episode 46 : -346.95652173913044  (epsilon 0.095)\n",
      "Episode 69 : -249.695652173913  (epsilon 0.09)\n",
      "Episode 92 : -196.6086956521739  (epsilon 0.08499999999999999)\n",
      "Episode 115 : -161.304347826087  (epsilon 0.07999999999999999)\n",
      "Episode 138 : -135.3913043478261  (epsilon 0.07499999999999998)\n",
      "Episode 161 : -116.26086956521739  (epsilon 0.06999999999999998)\n",
      "Episode 184 : -102.82608695652176  (epsilon 0.06499999999999997)\n",
      "Episode 207 : -89.17391304347827  (epsilon 0.05999999999999998)\n",
      "Episode 230 : -79.13043478260869  (epsilon 0.05499999999999998)\n",
      "Episode 253 : -71.43478260869566  (epsilon 0.04999999999999998)\n",
      "Episode 276 : -63.826086956521756  (epsilon 0.044999999999999984)\n",
      "Episode 299 : -58.73913043478261  (epsilon 0.03999999999999999)\n",
      "Episode 322 : -53.91304347826087  (epsilon 0.03499999999999999)\n",
      "Episode 345 : -50.56521739130435  (epsilon 0.02999999999999999)\n",
      "Episode 368 : -45.86956521739131  (epsilon 0.024999999999999988)\n",
      "Episode 391 : -44.8695652173913  (epsilon 0.019999999999999987)\n",
      "Episode 414 : -41.7391304347826  (epsilon 0.014999999999999986)\n",
      "Episode 437 : -41.43478260869565  (epsilon 0.009999999999999985)\n",
      "Episode 460 : -40.21739130434782  (epsilon 0.0049999999999999845)\n",
      "Episode 483 : -39.17391304347827  (epsilon -1.5612511283791264e-17)\n",
      "--------------------------------------------------\n",
      "Episode 23 : -964.478260869565  (epsilon 0.1)\n",
      "Episode 46 : -307.95652173913044  (epsilon 0.095)\n",
      "Episode 69 : -231.13043478260872  (epsilon 0.09)\n",
      "Episode 92 : -182.52173913043478  (epsilon 0.08499999999999999)\n",
      "Episode 115 : -153.30434782608694  (epsilon 0.07999999999999999)\n",
      "Episode 138 : -133.47826086956516  (epsilon 0.07499999999999998)\n",
      "Episode 161 : -117.47826086956522  (epsilon 0.06999999999999998)\n",
      "Episode 184 : -104.34782608695652  (epsilon 0.06499999999999997)\n",
      "Episode 207 : -94.30434782608694  (epsilon 0.05999999999999998)\n",
      "Episode 230 : -82.95652173913042  (epsilon 0.05499999999999998)\n",
      "Episode 253 : -76.34782608695653  (epsilon 0.04999999999999998)\n",
      "Episode 276 : -67.08695652173915  (epsilon 0.044999999999999984)\n",
      "Episode 299 : -62.0  (epsilon 0.03999999999999999)\n",
      "Episode 322 : -56.0  (epsilon 0.03499999999999999)\n",
      "Episode 345 : -51.56521739130435  (epsilon 0.02999999999999999)\n",
      "Episode 368 : -47.304347826086946  (epsilon 0.024999999999999988)\n",
      "Episode 391 : -45.95652173913044  (epsilon 0.019999999999999987)\n",
      "Episode 414 : -44.43478260869565  (epsilon 0.014999999999999986)\n",
      "Episode 437 : -41.260869565217384  (epsilon 0.009999999999999985)\n",
      "Episode 460 : -41.21739130434783  (epsilon 0.0049999999999999845)\n",
      "Episode 483 : -39.30434782608696  (epsilon -1.5612511283791264e-17)\n"
     ]
    }
   ],
   "source": [
    "maze = MazeEnv(\n",
    "    maze=np.array([[0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                   [0, 1, 0, 1, 0, 1, 1, 0, 1],\n",
    "                   [0, 1, 1, 1, 0, 1, 0, 0, 0],\n",
    "                   [0, 0, 0, 1, 0, 1, 1, 1, 0],\n",
    "                   [1, 1, 0, 1, 0, 0, 1, 0, 0],\n",
    "                   [0, 1, 0, 1, 1, 0, 1, 0, 1],\n",
    "                   [0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                   [1, 0, 1, 1, 0, 1, 1, 1, 0],\n",
    "                   [0, 0, 0, 0, 0, 0, 1, 2, 0]]),\n",
    "    start_pos=(0, 0)\n",
    ")\n",
    "\n",
    "agent = QLearning(\n",
    "     default_value = 0.,\n",
    "     learning_rate = 0.1,\n",
    "     reward_discount = 1.,\n",
    "     epsilon = 0.1\n",
    ")\n",
    "\n",
    "sarsa = SARSA()\n",
    "train_agent(env=maze, agent=sarsa, nb_episodes=500)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "qlearning = QLearning()\n",
    "agent = train_agent(env=maze, agent=qlearning, nb_episodes=500)\n",
    "\n",
    "demo_agent(maze, sarsa, gif_name='6_maze_sarsa.gif')\n",
    "demo_agent(maze, qlearning, gif_name='6_maze_qlearning.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

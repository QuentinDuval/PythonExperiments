{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - use a GYM\n",
    "# TODO - complexe... first without the maze, find the place to go\n",
    "\n",
    "# TODO - move through a maze where 1 are blocked, 0 are free, and you must find the end\n",
    "# TODO - use convolution net to find the right decision\n",
    "# TODO - reward is -1 for each time your are in the maze\n",
    "\n",
    "import abc\n",
    "from collections import *\n",
    "from dataclasses import dataclass\n",
    "import enum\n",
    "import numpy as np\n",
    "from typing import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Move(enum.Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "    \n",
    "    @staticmethod\n",
    "    def all():\n",
    "        return [Move.UP, Move.DOWN, Move.LEFT, Move.RIGHT]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class State:\n",
    "    i: int\n",
    "    j: int\n",
    "\n",
    "\n",
    "class MazeEnv:\n",
    "    def __init__(self, maze: np.ndarray, start_pos: Tuple[int, int], end_pos: Tuple[int, int]):\n",
    "        self.maze: np.ndarray = maze\n",
    "        self.start_pos: Tuple[int, int] = start_pos\n",
    "        self.end_pos: Tuple[int, int] = end_pos\n",
    "        self.i = self.start_pos[0]\n",
    "        self.j = self.start_pos[1]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.i = self.start_pos[0]\n",
    "        self.j = self.start_pos[1]\n",
    "        \n",
    "    def get_state(self) -> State:\n",
    "        return State(i=self.i, j=self.j)\n",
    "    \n",
    "    def get_actions(self) -> List[Move]:\n",
    "        return Move.all()\n",
    "    \n",
    "    def is_done(self) -> bool:\n",
    "        return self.i == self.end_pos[0] and self.j == self.end_pos[1]\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.is_done():\n",
    "            raise Exception(\"Game is over\")\n",
    "        \n",
    "        h, w = self.maze.shape\n",
    "        if action == Move.UP:\n",
    "            self.i = max(0, self.i - 1)\n",
    "        elif action == Move.DOWN:\n",
    "            self.i = min(h - 1, self.i + 1)\n",
    "        elif action == Move.LEFT:\n",
    "            self.j = max(0, self.j - 1)\n",
    "        elif action == Move.RIGHT:\n",
    "            self.j = min(w - 1, self.j + 1)\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionValues(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def get_actions(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def get_action_value(self, state, action) -> float:\n",
    "        pass\n",
    "    \n",
    "    def get_best_action(self, state):\n",
    "        best_action = None\n",
    "        best_score = float('-inf')\n",
    "        for action in self.get_actions(state):\n",
    "            score = self.get_action_value(state, action)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "\n",
    "class DiscreteActionValues(ActionValues):\n",
    "    def __init__(self, learning_rate: float = 0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.values = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    def add(self, state, action, score: float) -> float:\n",
    "        self.values[state][action] += self.learning_rate * (score - self.values[state][action])\n",
    "    \n",
    "    def get_actions(self, state):\n",
    "        return self.values[state].keys()\n",
    "    \n",
    "    def get_action_value(self, state, action) -> float:\n",
    "        return self.values[state][action]\n",
    "\n",
    "\n",
    "class SARSA:\n",
    "    def __init__(self,\n",
    "                 default_value: float = 0.,\n",
    "                 learning_rate: float = 0.1,\n",
    "                 reward_discount: float = 1.,\n",
    "                 epsilon: float = 0.1\n",
    "                ):\n",
    "        # a big default_value might help favoring exploration at early stages (but biase the results)\n",
    "        self.q_values = DiscreteActionValues(learning_rate)\n",
    "        self.reward_discount = reward_discount  # discount factor of future reward taken into account at present time\n",
    "        self.epsilon = epsilon                  # probability to take a random action\n",
    "    \n",
    "    def step(self, env) -> float:\n",
    "        state = env.get_state()\n",
    "        action = self._behavior_policy_action(env, state)\n",
    "        reward = env.step(action)\n",
    "        new_state = env.get_state()\n",
    "        new_action = self._target_policy_action(env, new_state)\n",
    "        score = reward + self.reward_discount * self.q_values.get_action_value(new_state, new_action)\n",
    "        self.q_values.add(state, action, score)\n",
    "        return reward\n",
    "    \n",
    "    def _target_policy_action(self, env, state):\n",
    "        return self._behavior_policy_action(env, state)\n",
    "    \n",
    "    def _behavior_policy_action(self, env, state):\n",
    "        if self.epsilon > 0. and np.random.random() < self.epsilon:\n",
    "            return np.random.choice(env.get_actions())\n",
    "        return self.q_values.get_best_action(state)\n",
    "\n",
    "\n",
    "class QLearning(SARSA):\n",
    "    def __init__(self,\n",
    "                 default_value: float = 0.,\n",
    "                 learning_rate: float = 0.1,\n",
    "                 reward_discount: float = 1.,\n",
    "                 epsilon: float = 0.1\n",
    "                ):\n",
    "        super().__init__(default_value=default_value, learning_rate=learning_rate, reward_discount=reward_discount, epsilon=epsilon)\n",
    "    \n",
    "    def _target_policy_action(self, env, state):\n",
    "        return self.q_values.get_best_action(state)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Training Loop\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class RunningAverage:\n",
    "    def __init__(self):\n",
    "        self.average = 0.\n",
    "        self.count = 0\n",
    "    \n",
    "    def add(self, value):\n",
    "        self.average += 1 / (self.count + 1) * (value - self.average)\n",
    "        self.count += 1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.average = 0.\n",
    "        self.count = 0\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.average\n",
    "    \n",
    "\n",
    "def simulate_episode(env, agent) -> float:\n",
    "    total_reward = 0.\n",
    "    env.reset()\n",
    "    while not env.is_done():\n",
    "        total_reward += agent.step(env)\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def train_agent(env, agent, nb_episodes: int):\n",
    "    running_average = RunningAverage()\n",
    "    temperature_decrease_period = nb_episodes // 21\n",
    "    temperature_decrease = agent.epsilon / 20\n",
    "    for episode in range(1, nb_episodes + 1):\n",
    "        reward = simulate_episode(env, agent)\n",
    "        running_average.add(reward)\n",
    "        if episode % temperature_decrease_period == 0:\n",
    "            print(\"Episode\", episode, \":\", running_average(), \" (epsilon \" + str(agent.epsilon) + \")\")\n",
    "            agent.epsilon -= temperature_decrease\n",
    "            running_average.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23 : -63.3478260869565  (epsilon 0.1)\n",
      "Episode 46 : -26.434782608695652  (epsilon 0.095)\n",
      "Episode 69 : -15.56521739130435  (epsilon 0.09)\n",
      "Episode 92 : -14.217391304347826  (epsilon 0.08499999999999999)\n",
      "Episode 115 : -15.304347826086955  (epsilon 0.07999999999999999)\n",
      "Episode 138 : -12.695652173913043  (epsilon 0.07499999999999998)\n",
      "Episode 161 : -9.217391304347826  (epsilon 0.06999999999999998)\n",
      "Episode 184 : -9.826086956521738  (epsilon 0.06499999999999997)\n",
      "Episode 207 : -9.869565217391305  (epsilon 0.05999999999999998)\n",
      "Episode 230 : -9.21739130434783  (epsilon 0.05499999999999998)\n",
      "Episode 253 : -8.826086956521744  (epsilon 0.04999999999999998)\n",
      "Episode 276 : -8.695652173913041  (epsilon 0.044999999999999984)\n",
      "Episode 299 : -7.956521739130436  (epsilon 0.03999999999999999)\n",
      "Episode 322 : -8.260869565217392  (epsilon 0.03499999999999999)\n",
      "Episode 345 : -7.869565217391305  (epsilon 0.02999999999999999)\n",
      "Episode 368 : -7.739130434782608  (epsilon 0.024999999999999988)\n",
      "Episode 391 : -7.608695652173914  (epsilon 0.019999999999999987)\n",
      "Episode 414 : -7.434782608695652  (epsilon 0.014999999999999986)\n",
      "Episode 437 : -7.304347826086956  (epsilon 0.009999999999999985)\n",
      "Episode 460 : -7.347826086956523  (epsilon 0.0049999999999999845)\n",
      "Episode 483 : -7.217391304347827  (epsilon -1.5612511283791264e-17)\n"
     ]
    }
   ],
   "source": [
    "maze = MazeEnv(\n",
    "    maze=np.array([[0, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 0, 0]]),\n",
    "    start_pos=(0, 0),\n",
    "    end_pos=(3, 4)\n",
    ")\n",
    "\n",
    "agent = QLearning(\n",
    "     default_value = 0.,\n",
    "     learning_rate = 0.1,\n",
    "     reward_discount = 1.,\n",
    "     epsilon = 0.1\n",
    ")\n",
    "\n",
    "train_agent(env=maze, agent=QLearning(), nb_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

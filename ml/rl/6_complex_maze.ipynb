{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from collections import *\n",
    "from dataclasses import dataclass\n",
    "import enum\n",
    "import gym\n",
    "import numpy as np\n",
    "from typing import *\n",
    "\n",
    "%matplotlib inline\n",
    "import imageio\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pyglet\n",
    "from pyglet.gl import *\n",
    "\n",
    "window = pyglet.window.Window(width=300, height=200, display=None)\n",
    "window.clear()\n",
    "\n",
    "from gym.envs.classic_control import rendering\n",
    "viewer = rendering.Viewer(screen_width, screen_height)\n",
    "'''\n",
    "\n",
    "# TODO - complexe... first without the maze, find the place to go\n",
    "\n",
    "# TODO - move through a maze where 1 are blocked, 0 are free, and you must find the end\n",
    "# TODO - use convolution net to find the right decision\n",
    "# TODO - reward is -1 for each time your are in the maze\n",
    "\n",
    "# TODO - make a custom space for actions / states?\n",
    "\n",
    "\n",
    "class MazeEnv(gym.Env):\n",
    "    def __init__(self, maze: np.ndarray, start_pos: Tuple[int, int], end_pos: Tuple[int, int]):\n",
    "        self.maze: np.ndarray = maze\n",
    "        self.start_pos: Tuple[int, int] = start_pos\n",
    "        self.end_pos: Tuple[int, int] = end_pos\n",
    "        self.state = None # Must be immutable? I guess so\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space = gym.spaces.MultiDiscrete(list(maze.shape))\n",
    "    \n",
    "    @property\n",
    "    def done(self) -> bool:\n",
    "        return self.state == self.end_pos\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = self.start_pos\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            raise Exception(\"Game is over\")\n",
    "        self._move(action)\n",
    "        return self.state, -1, self.done, {}\n",
    "\n",
    "    def _move(self, action):\n",
    "        i, j = self.state\n",
    "        h, w = self.maze.shape\n",
    "        if action == 0:   # UP\n",
    "            i = max(0, i - 1)\n",
    "        elif action == 1: # DOWN\n",
    "            i = min(h - 1, i + 1)\n",
    "        elif action == 2: # LEFT\n",
    "            j = max(0, j - 1)\n",
    "        elif action == 3: # RIGHT\n",
    "            j = min(w - 1, j + 1)\n",
    "        if self.maze[i, j] == 0:\n",
    "            self.state = (i, j)\n",
    "    \n",
    "    def render_state(self, zoom: int):\n",
    "        h, w = self.maze.shape\n",
    "        m = np.zeros((h, w, 3), 'uint8')\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                if self.maze[i, j] == 1:\n",
    "                    m[i, j, 0] = 255\n",
    "        i, j = self.state\n",
    "        m[i, j, 2] = 255\n",
    "        image = Image.fromarray(m, 'RGB')\n",
    "        image = image.resize((w * zoom, h * zoom))\n",
    "        return image        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionValues(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def get_actions(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def get_action_value(self, state, action) -> float:\n",
    "        pass\n",
    "    \n",
    "    def get_best_action(self, state):\n",
    "        best_action = None\n",
    "        best_score = float('-inf')\n",
    "        for action in self.get_actions(state):\n",
    "            score = self.get_action_value(state, action)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "\n",
    "class DiscreteActionValues(ActionValues):\n",
    "    def __init__(self, learning_rate: float = 0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.values = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    def add(self, state, action, score: float) -> float:\n",
    "        self.values[state][action] += self.learning_rate * (score - self.values[state][action])\n",
    "    \n",
    "    def get_actions(self, state):\n",
    "        return self.values[state].keys()\n",
    "    \n",
    "    def get_action_value(self, state, action) -> float:\n",
    "        return self.values[state][action]\n",
    "\n",
    "\n",
    "class SARSA:\n",
    "    def __init__(self,\n",
    "                 default_value: float = 0.,\n",
    "                 learning_rate: float = 0.1,\n",
    "                 reward_discount: float = 1.,\n",
    "                 epsilon: float = 0.1\n",
    "                ):\n",
    "        # a big default_value might help favoring exploration at early stages (but biase the results)\n",
    "        self.q_values = DiscreteActionValues(learning_rate)\n",
    "        self.reward_discount = reward_discount  # discount factor of future reward taken into account at present time\n",
    "        self.epsilon = epsilon                  # probability to take a random action\n",
    "    \n",
    "    def play_episode(self, env) -> float:\n",
    "        total_reward = 0.\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self._behavior_policy_action(env, state)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            new_action = self._target_policy_action(env, new_state)\n",
    "            score = reward + self.reward_discount * self.q_values.get_action_value(new_state, new_action)\n",
    "            self.q_values.add(state, action, score)\n",
    "            total_reward += reward\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "    \n",
    "    def get_action(self, env, state):\n",
    "        return self._target_policy_action(env, state)\n",
    "    \n",
    "    def _target_policy_action(self, env, state):\n",
    "        return self._behavior_policy_action(env, state)\n",
    "    \n",
    "    def _behavior_policy_action(self, env, state):\n",
    "        if self.epsilon > 0. and np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        return self.q_values.get_best_action(state)\n",
    "\n",
    "\n",
    "class QLearning(SARSA):\n",
    "    def __init__(self,\n",
    "                 default_value: float = 0.,\n",
    "                 learning_rate: float = 0.1,\n",
    "                 reward_discount: float = 1.,\n",
    "                 epsilon: float = 0.1\n",
    "                ):\n",
    "        super().__init__(default_value=default_value, learning_rate=learning_rate, reward_discount=reward_discount, epsilon=epsilon)\n",
    "    \n",
    "    def _target_policy_action(self, env, state):\n",
    "        return self.q_values.get_best_action(state)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Training Loop\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class RunningAverage:\n",
    "    def __init__(self):\n",
    "        self.average = 0.\n",
    "        self.count = 0\n",
    "    \n",
    "    def add(self, value):\n",
    "        self.average += 1 / (self.count + 1) * (value - self.average)\n",
    "        self.count += 1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.average = 0.\n",
    "        self.count = 0\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.average\n",
    "\n",
    "\n",
    "def train_agent(env, agent, nb_episodes: int):\n",
    "    running_average = RunningAverage()\n",
    "    temperature_decrease_period = nb_episodes // 21\n",
    "    temperature_decrease = agent.epsilon / 20\n",
    "    for episode in range(1, nb_episodes + 1):\n",
    "        reward = agent.play_episode(env)\n",
    "        running_average.add(reward)\n",
    "        if episode % temperature_decrease_period == 0:\n",
    "            print(\"Episode\", episode, \":\", running_average(), \" (epsilon \" + str(agent.epsilon) + \")\")\n",
    "            agent.epsilon -= temperature_decrease\n",
    "            running_average.reset()\n",
    "    return agent\n",
    "\n",
    "\n",
    "def demo_agent(env, agent, gif_name: str):\n",
    "    images = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(env, state)\n",
    "        images.append(env.render_state(10))\n",
    "        state, reward, done, info = env.step(action)\n",
    "    images.append(env.render_state(10))\n",
    "    imageio.mimsave(gif_name, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23 : -159.21739130434784  (epsilon 0.1)\n",
      "Episode 46 : -70.21739130434781  (epsilon 0.095)\n",
      "Episode 69 : -49.78260869565218  (epsilon 0.09)\n",
      "Episode 92 : -36.26086956521739  (epsilon 0.08499999999999999)\n",
      "Episode 115 : -28.43478260869565  (epsilon 0.07999999999999999)\n",
      "Episode 138 : -23.26086956521739  (epsilon 0.07499999999999998)\n",
      "Episode 161 : -20.999999999999993  (epsilon 0.06999999999999998)\n",
      "Episode 184 : -18.391304347826086  (epsilon 0.06499999999999997)\n",
      "Episode 207 : -16.82608695652174  (epsilon 0.05999999999999998)\n",
      "Episode 230 : -16.30434782608696  (epsilon 0.05499999999999998)\n",
      "Episode 253 : -16.869565217391305  (epsilon 0.04999999999999998)\n",
      "Episode 276 : -16.217391304347828  (epsilon 0.044999999999999984)\n",
      "Episode 299 : -16.08695652173913  (epsilon 0.03999999999999999)\n",
      "Episode 322 : -15.695652173913041  (epsilon 0.03499999999999999)\n",
      "Episode 345 : -15.478260869565217  (epsilon 0.02999999999999999)\n",
      "Episode 368 : -15.739130434782608  (epsilon 0.024999999999999988)\n",
      "Episode 391 : -15.260869565217392  (epsilon 0.019999999999999987)\n",
      "Episode 414 : -15.086956521739129  (epsilon 0.014999999999999986)\n",
      "Episode 437 : -15.086956521739129  (epsilon 0.009999999999999985)\n",
      "Episode 460 : -15.17391304347826  (epsilon 0.0049999999999999845)\n",
      "Episode 483 : -15.0  (epsilon -1.5612511283791264e-17)\n",
      "--------------------------------------------------\n",
      "Episode 23 : -202.26086956521743  (epsilon 0.1)\n",
      "Episode 46 : -60.434782608695656  (epsilon 0.095)\n",
      "Episode 69 : -35.69565217391305  (epsilon 0.09)\n",
      "Episode 92 : -23.652173913043477  (epsilon 0.08499999999999999)\n",
      "Episode 115 : -25.65217391304348  (epsilon 0.07999999999999999)\n",
      "Episode 138 : -23.869565217391308  (epsilon 0.07499999999999998)\n",
      "Episode 161 : -21.043478260869566  (epsilon 0.06999999999999998)\n",
      "Episode 184 : -18.826086956521735  (epsilon 0.06499999999999997)\n",
      "Episode 207 : -17.999999999999996  (epsilon 0.05999999999999998)\n",
      "Episode 230 : -16.695652173913043  (epsilon 0.05499999999999998)\n",
      "Episode 253 : -16.043478260869563  (epsilon 0.04999999999999998)\n",
      "Episode 276 : -15.826086956521737  (epsilon 0.044999999999999984)\n",
      "Episode 299 : -16.043478260869566  (epsilon 0.03999999999999999)\n",
      "Episode 322 : -15.956521739130434  (epsilon 0.03499999999999999)\n",
      "Episode 345 : -15.956521739130432  (epsilon 0.02999999999999999)\n",
      "Episode 368 : -15.608695652173914  (epsilon 0.024999999999999988)\n",
      "Episode 391 : -15.43478260869565  (epsilon 0.019999999999999987)\n",
      "Episode 414 : -15.086956521739129  (epsilon 0.014999999999999986)\n",
      "Episode 437 : -15.17391304347826  (epsilon 0.009999999999999985)\n",
      "Episode 460 : -15.08695652173913  (epsilon 0.0049999999999999845)\n",
      "Episode 483 : -15.0  (epsilon -1.5612511283791264e-17)\n"
     ]
    }
   ],
   "source": [
    "maze = MazeEnv(\n",
    "    maze=np.array([[0, 1, 0, 0, 1, 0],\n",
    "                   [0, 1, 0, 0, 0, 0],\n",
    "                   [0, 1, 0, 1, 0, 0],\n",
    "                   [0, 1, 0, 0, 1, 0],\n",
    "                   [0, 0, 0, 1, 0, 0]]),\n",
    "    start_pos=(0, 0),\n",
    "    end_pos=(4, 5)\n",
    ")\n",
    "\n",
    "agent = QLearning(\n",
    "     default_value = 0.,\n",
    "     learning_rate = 0.1,\n",
    "     reward_discount = 1.,\n",
    "     epsilon = 0.1\n",
    ")\n",
    "\n",
    "sarsa = SARSA()\n",
    "train_agent(env=maze, agent=sarsa, nb_episodes=500)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "qlearning = QLearning()\n",
    "agent = train_agent(env=maze, agent=qlearning, nb_episodes=500)\n",
    "\n",
    "demo_agent(maze, sarsa, gif_name='6_maze_sarsa.gif')\n",
    "demo_agent(maze, qlearning, gif_name='6_maze_qlearning.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

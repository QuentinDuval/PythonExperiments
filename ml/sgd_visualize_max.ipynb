{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.41660404205322\n",
      "0.00031899770328891464\n",
      "0.00012811376905119687\n",
      "0.00011882758644787828\n",
      "0.019658473684103228\n",
      "0.8433895418420434\n",
      "0.025898572814185172\n",
      "0.003969318553572521\n",
      "7.839351525262828e-06\n",
      "0.0025544871932652313\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Even the simple XOR system has 8 dimensions of evolutions:\n",
    "- This model does not always learn\n",
    "- Highly depends on the initialization\n",
    "- The minibatch is not helping here (not enough inputs: no estimation of gradient)\n",
    "\"\"\"\n",
    "\n",
    "class MaximumModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(2, 2), nn.ReLU(), nn.Linear(2, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "def train(xs, ys):\n",
    "    data_set = TensorDataset(xs, ys)\n",
    "    data_loader = DataLoader(data_set, batch_size=100, shuffle=True)\n",
    "\n",
    "    model = MaximumModel()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-1)\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        cumulative_loss = 0.\n",
    "        for inputs, expected in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            got = model(inputs)\n",
    "            loss = criterion(got, expected.unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumulative_loss += loss.item()\n",
    "        if epoch % 100 == 0:\n",
    "            print(cumulative_loss)\n",
    "    return model\n",
    "\n",
    "n = 1000\n",
    "xs = np.random.uniform(-5, 5, size=2*n).reshape((n, 2))\n",
    "ys = np.apply_along_axis(lambda x: max(x), axis=-1, arr=xs)\n",
    "\n",
    "xs = torch.tensor(xs, dtype=torch.float32, requires_grad=False)\n",
    "ys = torch.tensor(ys, dtype=torch.float32, requires_grad=False)\n",
    "model = train(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5691013]\n",
      " [-1.4501865]\n",
      " [ 3.1062284]]\n",
      "[[-4.6807637]\n",
      " [11.306477 ]\n",
      " [10.024773 ]]\n"
     ]
    }
   ],
   "source": [
    "def predict(model, xs):\n",
    "    xs = torch.tensor(xs, dtype=torch.float32, requires_grad=False)\n",
    "    ys = model(xs)\n",
    "    return ys.detach().numpy()\n",
    "\n",
    "# Works great for inputs it has seen\n",
    "print(predict(model, [[1.5, -1.5], [-1.5, -2.5], [2, 3]]))\n",
    "\n",
    "# Does not work that great outside of the training interval\n",
    "print(predict(model, [[-10, -11], [10, 11], [10, -10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.232817992568016\n",
      "0.2702178319450468\n",
      "0.18278778492094716\n",
      "0.15379493188811466\n",
      "0.11609094208688475\n",
      "0.18232797240489163\n",
      "0.06846127461176366\n",
      "0.05554319115799444\n",
      "0.42604836699320003\n",
      "0.06338563692406751\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Same idea, but this time the classifier will just return the index of the maximum value\n",
    "\"\"\"\n",
    "\n",
    "class MaximumIndexModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(2, 2), nn.ReLU(), nn.Linear(2, 2))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x, with_softmax=True):\n",
    "        x = self.model(x)\n",
    "        if with_softmax:\n",
    "            x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "def train(xs, ys):\n",
    "    data_set = TensorDataset(xs, ys)\n",
    "    data_loader = DataLoader(data_set, batch_size=100, shuffle=True)\n",
    "\n",
    "    model = MaximumIndexModel()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-1)\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        cumulative_loss = 0.\n",
    "        for inputs, expected in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            got = model(inputs, with_softmax=False)\n",
    "            loss = criterion(got, expected)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumulative_loss += loss.item()\n",
    "        if epoch % 100 == 0:\n",
    "            print(cumulative_loss)\n",
    "    return model\n",
    "\n",
    "n = 1000\n",
    "xs = np.random.uniform(-5, 5, size=2*n).reshape((n, 2))\n",
    "ys = np.apply_along_axis(lambda x: 0 if x[0] >= x[1] else 1, axis=-1, arr=xs)\n",
    "\n",
    "xs = torch.tensor(xs, dtype=torch.float32, requires_grad=False)\n",
    "ys = torch.tensor(ys, dtype=torch.long, requires_grad=False)\n",
    "model = train(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5, -1.5, 3]\n",
      "[-10, 11, 10]\n"
     ]
    }
   ],
   "source": [
    "def predict(model, inputs):\n",
    "    xs = torch.tensor(inputs, dtype=torch.float32, requires_grad=False)\n",
    "    ys = model(xs)\n",
    "    ys = torch.argmax(xs, dim=-1)\n",
    "    return [inputs[i][j] for i, j in enumerate(ys)]\n",
    "\n",
    "# Now it works great for any kind of inputs\n",
    "print(predict(model, [[1.5, -1.5], [-1.5, -2.5], [2, 3]]))\n",
    "print(predict(model, [[-10, -11], [10, 11], [10, -10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126.64274597167969\n",
      "19.927125096321106\n",
      "7.24814248085022\n",
      "2.0433948636054993\n",
      "0.9851384460926056\n",
      "0.5886765159666538\n",
      "0.41289015114307404\n",
      "0.30657883174717426\n",
      "0.24300667084753513\n",
      "0.19793241377919912\n",
      "0.168237435631454\n",
      "0.14474424347281456\n",
      "0.1311894878745079\n",
      "0.11536591360345483\n",
      "0.10669053392484784\n",
      "0.09784344397485256\n",
      "0.0909389122389257\n",
      "0.08726467099040747\n",
      "0.08494188683107495\n",
      "0.07780324993655086\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Same idea, but this time, we use a sequence of any kind of length (using LSTM)\n",
    "\"\"\"\n",
    "\n",
    "class MaximumModelRange(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 2\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=self.hidden_size)\n",
    "        self.fc = nn.Linear(self.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(dim=2)\n",
    "        x = torch.transpose(x, dim0=0, dim1=1)\n",
    "        seq_len, batch_size, input_size = x.shape\n",
    "        # print(x.shape)\n",
    "        \n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        # print(output.shape)\n",
    "        # print(output[-1].shape)\n",
    "        \n",
    "        ys = self.fc(output[-1]) # Take the last output, and feed it to the linear layer\n",
    "        ys = ys.squeeze(dim=-1)  # Get rid of the last value of dimension 1 (regression)\n",
    "        return ys\n",
    "\n",
    "# Can be used like this\n",
    "#   xs = torch.tensor([[1, 2, 3, 4], [2, 1, 4, 3], [-11, 11, 12, -12]], dtype = torch.float32, requires_grad=False)  \n",
    "#   model = MaximumModelRange()\n",
    "#   model(xs)\n",
    "# Will output:\n",
    "#   tensor([0.1261, 0.1411, 0.2904], grad_fn=<SqueezeBackward1>)\n",
    "\n",
    "def train_lstm(xs, ys):\n",
    "    data_set = TensorDataset(xs, ys)\n",
    "    data_loader = DataLoader(data_set, batch_size=100, shuffle=True)\n",
    "\n",
    "    model = MaximumModelRange()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(2000):\n",
    "        cumulative_loss = 0.\n",
    "        for inputs, expected in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            got = model(inputs)\n",
    "            loss = criterion(got, expected)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumulative_loss += loss.item()\n",
    "        if epoch % 100 == 0:\n",
    "            print(cumulative_loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "n = 1000\n",
    "xs = np.random.uniform(-5, 5, size=4*n).reshape((n, 4))\n",
    "ys = np.apply_along_axis(lambda x: max(x), axis=-1, arr=xs)\n",
    "\n",
    "xs = torch.tensor(xs, dtype=torch.float32, requires_grad=False)\n",
    "ys = torch.tensor(ys, dtype=torch.float32, requires_grad=False)\n",
    "model = train_lstm(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.9640284  -0.84462476  5.0479846 ]\n"
     ]
    }
   ],
   "source": [
    "def predict_lstm(model, xs):\n",
    "    xs = torch.tensor(xs, dtype=torch.float32, requires_grad=False)\n",
    "    ys = model(xs)\n",
    "    return ys.detach().numpy()\n",
    "\n",
    "# Works \"okay\" for inputs that fall into the range\n",
    "print(predict_lstm(model, [[1.5, -1.5, 3], [-1.5, -2.5, -3.5], [2, 5, 3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

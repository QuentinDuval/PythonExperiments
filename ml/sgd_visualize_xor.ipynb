{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7094878554344177\n",
      "0.003522902727127075\n",
      "0.0013992488384246826\n",
      "0.0007776021957397461\n",
      "0.0005016326904296875\n",
      "0.00035318732261657715\n",
      "0.0002639591693878174\n",
      "0.0002047419548034668\n",
      "0.00016415119171142578\n",
      "0.00013449788093566895\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Even the simple XOR system has 8 dimensions of evolutions:\n",
    "- This model does not always learn\n",
    "- Highly depends on the initialization\n",
    "- The minibatch is not helping here (not enough inputs: no estimation of gradient)\n",
    "\"\"\"\n",
    "\n",
    "class XORModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(2, 2), nn.ReLU(), nn.Linear(2, 2))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.xavier_normal_(module.weight)\n",
    "    \n",
    "    def forward(self, x, with_softmax=True):\n",
    "        x = x.float()\n",
    "        x = self.model(x)\n",
    "        if with_softmax:\n",
    "            x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "def train(xs, ys):\n",
    "    data_set = TensorDataset(xs, ys)\n",
    "    data_loader = DataLoader(data_set, batch_size=4, shuffle=True)\n",
    "\n",
    "    model = XORModel()\n",
    "    model.init_weights()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-1)\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        cumulative_loss = 0.\n",
    "        for inputs, expected in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            got = model(inputs, with_softmax=False)\n",
    "            loss = criterion(got, expected)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumulative_loss += loss.item()\n",
    "        if epoch % 100 == 0:\n",
    "            print(cumulative_loss)\n",
    "    return model\n",
    "\n",
    "xs = torch.tensor([[0, 0], [1, 0], [0, 1], [1, 1]], dtype=torch.long, requires_grad=False)\n",
    "ys = torch.tensor([0, 1, 1, 0], dtype=torch.long, requires_grad=False)\n",
    "model = train(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify(model, xs):\n",
    "    xs = torch.tensor(xs, dtype=torch.long, requires_grad=False)\n",
    "    ys = model(xs, with_softmax=True)\n",
    "    return torch.argmax(ys, dim=-1)\n",
    "\n",
    "classify(model, [[0, 0], [1, 0], [0, 1], [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7019362449645996\n",
      "0.005526186432689428\n",
      "0.00216383533552289\n",
      "0.0011967032914981246\n",
      "0.0007712696678936481\n",
      "0.0005452843615785241\n",
      "0.00040775196976028383\n",
      "0.000318303209496662\n",
      "0.00025565517717041075\n",
      "0.0002100435522152111\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reducing the number of dimensions by using a simple Sigmoid, and BCELoss\n",
    "\"\"\"\n",
    "\n",
    "class XORModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(2, 2), nn.ReLU(), nn.Linear(2, 1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.xavier_normal_(module.weight)\n",
    "    \n",
    "    def forward(self, x, with_sigmoid=True):\n",
    "        x = x.float()\n",
    "        x = self.model(x)\n",
    "        if with_sigmoid:\n",
    "            x = self.sigmoid(x)\n",
    "        else:\n",
    "            x = \n",
    "        return x\n",
    "\n",
    "def train(xs, ys):\n",
    "    data_set = TensorDataset(xs, ys)\n",
    "    data_loader = DataLoader(data_set, batch_size=4, shuffle=True)\n",
    "\n",
    "    model = XORModel()\n",
    "    model.init_weights()\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss() # Numerically stable version of nn.Sigmoid() + nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-1)\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        cumulative_loss = 0.\n",
    "        for inputs, expected in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            got = model(inputs, with_sigmoid=False)\n",
    "            loss = criterion(got, expected)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumulative_loss += loss.item()\n",
    "        if epoch % 100 == 0:\n",
    "            print(cumulative_loss)\n",
    "    return model\n",
    "\n",
    "xs = torch.tensor([[0, 0], [1, 0], [0, 1], [1, 1]], dtype=torch.long, requires_grad=False)\n",
    "ys = torch.tensor([0, 1, 1, 0], dtype=torch.float, requires_grad=False)\n",
    "model2 = train(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, True, False]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify(model, xs):\n",
    "    xs = torch.tensor(xs, dtype=torch.long, requires_grad=False)\n",
    "    ys = model(xs)\n",
    "    return list(y.item() >= 0.5 for y in ys)\n",
    "\n",
    "classify(model2, [[0, 0], [1, 0], [0, 1], [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
